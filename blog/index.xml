<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>andrewlook</title>
<link>https://andrewlook.com/notebooks/blog/</link>
<atom:link href="https://andrewlook.com/notebooks/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Andrew Look Personal Website</description>
<generator>quarto-1.6.42</generator>
<lastBuildDate>Thu, 04 Jan 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>SLML Part 7 - SketchRNN Experiments: Data Augmentation</title>
  <dc:creator>Andrew Look</dc:creator>
  <link>https://andrewlook.com/notebooks/blog/posts/slml_part7.html</link>
  <description><![CDATA[ 




<section id="slml-part-7---sketchrnn-experiments-data-augmentation" class="level1 page-columns page-full">
<h1>SLML Part 7 - SketchRNN Experiments: Data Augmentation</h1>
<blockquote class="blockquote">
<p><em>This post is part 7 of <a href="../../projects/slml.html">“SLML” - Single-Line Machine Learning</a>.</em></p>
<p><em>To read the previous post, check out <a href="../../blog/posts/slml_part6.html">part 6</a>.</em></p>
</blockquote>
<!--
>
> _If you want to keep reading, check out [part 8](./slml_part8.qmd)._
-->
<section id="preprocessing-for-data-augmentation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="preprocessing-for-data-augmentation">Preprocessing for Data Augmentation</h2>
<p>I was curious about why my earlier <a href="../../slml_part4.html#recurrent-dropout">experiment using stroke augmentation</a> didn’t show benefits (and in some cases made the models perform much worse on validation metrics). In Figure&nbsp;1, it’s clear that the pink line (without stroke augmentation) has a faster-growing validation loss after the point of overfitting.</p>
<div id="fig-recurrentdropout" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-recurrentdropout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="https://i.ibb.co/xJD1VMg/phase2-wandb-V2-recurrent-dropout.png" class="img-fluid figure-img column-body-outset">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-recurrentdropout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Training and validation <a href="https://wandb.ai/andrewlook/sketchrnn-pytorch/reports/look_i16_minn10-layernorm-with-without-recurrent-dropout--Vmlldzo2OTQ1Mzk5">loss metrics</a> for layernorm-only models (burgundy and gray) alongside models trained with recurrent dropout (green) and with stroke augmentation (pink).
</figcaption>
</figure>
</div>
<p>Those experiments used stroke augmentation as a hyperparameter on the dataset - so at runtime when the model started training, it would modify the drawings before assembling them into batches for the trainer.</p>
<div class="page-columns page-full"><p>I decided to create a dataset where I ran the augmentation ahead of time and saved the result so I could inspect the results more closely. For each entry in the dataset, I included the original drawing as well as a seperate entry for the drawing in reverse with a “flipped” sequence. This doubled the size of the dataset. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">Another idea I’d like to try is applying local distortions or dilations, since that would change the directions certain strokes take without losing the overal subject of the drawing. <a href="https://en.wikipedia.org/wiki/Radial_basis_function">Radial Basis Functions</a> or <a href="https://www.tvpaint.com/doc/tvp11/index.php?id=lesson-fx-distortion-warp-grid">Warp Grids</a> seem like promising approaches to try.</span></div></div>
<p>Then I took each drawing and randomly:</p>
<ul>
<li>Applied stroke agumentation to drop points, with a probability up to 0.5.</li>
<li>Randomly rotated -15 to 15 degrees</li>
<li>Randomly scaled between 100% to 120% of original size.</li>
</ul>
<p>Some examples of the augmentations are visible in Figure&nbsp;2.</p>
<div id="fig-data-aug" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-aug-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.ibb.co/sq9YtDX/aug0.png" class="img-fluid figure-img"></p>
<figcaption>Original drawing</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.ibb.co/VDp5d3D/aug1.png" class="img-fluid figure-img"></p>
<figcaption>After “Stroke Augmentation” drops points from lines at random</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.ibb.co/z59gv5S/aug2.png" class="img-fluid figure-img"></p>
<figcaption>Randomly rotated and scaled</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-aug-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Examples of data augmentation.
</figcaption>
</figure>
</div>
</section>
<section id="training-with-augmented-dataset" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="training-with-augmented-dataset">Training With Augmented Dataset</h2>
<p>Comparing the validation loss metrics in Figure&nbsp;3 from the models trained on augmented dataset (purple) with my previous round of best-performing models, the augmented dataset takes longer to converge but the validation loss keeps sloping downwards. This is encouraging to me since it seems like the more diverse drawings in the augmented dataset are helping the model learn to generalize more than pervious models trained on non-augmented datasets.</p>
<div id="fig-train-aug" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-train-aug-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="https://i.ibb.co/jbJ7RQF/phase6-wandb-epoch20240221-data-aug.png" class="img-fluid figure-img column-body-outset">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-train-aug-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Training and validation <a href="https://wandb.ai/andrewlook/sketchrnn-pytorch/reports/epoch20240221_expanded10x_trainval--Vmlldzo2OTQ1NjUw">loss metrics</a> from augmented dataset <code>20240221-dataaug10x</code> (purple).
</figcaption>
</figure>
</div>
<p>As a small side experiment, I wanted to confirm my finding in <a href="../../blog/posts/slml_part4.html">part 4</a> that layer norm caused a meaningful improvement. Looking at the loss metrics in Figure&nbsp;4, it appears that disabling layernorm (light green) causes a significant drop in performance. Disabling recurrent dropout doesn’t have a significant effect, as far as I can tell.</p>
<div id="fig-train-aug-ln" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-train-aug-ln-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="https://i.ibb.co/5L7BkNM/phase6-wandb-without-ln-rd.png" class="img-fluid figure-img column-body-outset"></p>
<figcaption>phase6-wandb-without-ln-rd</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-train-aug-ln-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Training and validation <a href="https://wandb.ai/andrewlook/sketchrnn-pytorch/reports/epoch20240221-with-without-layernorm-recurrent-dropout--Vmlldzo2OTQ1NzA0">loss metrics</a> from augmented dataset <code>20240221-dataaug10x</code> (purple) compared to variants without layernorm (light green) and without recurrent dropout (magenta / dark red).
</figcaption>
</figure>
</div>
<!--
> _If you want to keep reading, check out [part 7](./slml_part7.qmd) of my [SLML](/projects/slml.qmd) series._
-->
<!--

=== Phase 6 - Data Aug ===

* 1to0qyp3: [enchanting-fireworks-40\_\_dataaug10x | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/1to0qyp3?workspace=user-andrewlook)
* 5m3e5ent: [auspicious-dragon-43\_\_dataaug10x\_bestval | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/5m3e5ent?workspace=user-andrewlook)
    * note: identical hyperparams, but I had the model set up to save every 100 epochs. Since the much larger augmented dataset had longer epochs, the point of overfitting came around epoch
    * ![phase6-wandb-overfitting-point](https://i.ibb.co/bKkFsG6/phase6-wandb-overfitting-point.png)
-->


</section>
</section>

 ]]></description>
  <category>singleline</category>
  <category>machinelearning</category>
  <category>slml</category>
  <guid>https://andrewlook.com/notebooks/blog/posts/slml_part7.html</guid>
  <pubDate>Thu, 04 Jan 2024 00:00:00 GMT</pubDate>
  <media:content url="https://i.ibb.co/BPdSrTQ/phase1-wandb-with-without-minn10.png" medium="image" type="image/png"/>
</item>
<item>
  <title>SLML Part 6 - SketchRNN Experiments: Granular Visual Filtering</title>
  <dc:creator>Andrew Look</dc:creator>
  <link>https://andrewlook.com/notebooks/blog/posts/slml_part6.html</link>
  <description><![CDATA[ 




<section id="slml-part-6---sketchrnn-experiments-granular-visual-filtering" class="level1 page-columns page-full">
<h1>SLML Part 6 - SketchRNN Experiments: Granular Visual Filtering</h1>
<blockquote class="blockquote">
<p><em>This post is part 6 of <a href="../../projects/slml.html">“SLML” - Single-Line Machine Learning</a>.</em></p>
<p><em>To read the previous post, check out <a href="../../blog/posts/slml_part5.html">part 5</a>.</em></p>
<p><em>If you want to keep reading, check out <a href="../../blog/posts/slml_part7.html">part 7</a>.</em></p>
</blockquote>
<section id="new-dataset-20240104" class="level2">
<h2 class="anchored" data-anchor-id="new-dataset-20240104">New Dataset: <code>20240104</code></h2>
<p>Though I’d grown my training dataset by bounding-box separating single pages into multiple drawings, I was concerned about the tradeoff of filtering drawings out versus having a more coherent dataset with similar subject matter.</p>
<p>To make up for more aggressive filtering, I decided to incorporate several additional sketchbooks I scanned and labeled into a new dataset epoch <code>20240104</code>.</p>
<p>Differences in dataset <code>20240104</code> compared to <a href="../../blog/posts/slml_part5.html#dataset-20231214">dataset <code>20231214</code></a>:</p>
<ol type="1">
<li>More raw input drawings</li>
<li>Same preprocessing, with a modified “adaptive” RDP simplification <sup>1</sup>.</li>
</ol>
</section>
<section id="rdp-and-sequence-length" class="level2">
<h2 class="anchored" data-anchor-id="rdp-and-sequence-length">RDP and Sequence Length</h2>
<p>In previous datasets, I had chosen the same strength of RDP line simplification for the whole dataset. Some drawings had been simplified reasonably, but other had been simple to begin with and ended up as a series of straight lines much sharper than the original curves.</p>
<div id="fig-30" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-30-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/2Pw6zvG/30points.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-30-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: 30 points
</figcaption>
</figure>
</div>
<p>For the remaining drawings, I ran the RDP algorithm with varying values for its <code>epsilon</code> parameter, until the number of points dipped under 250. Then I saved the result as a zipped numpy file.</p>
</section>
<section id="training-on-20240104" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="training-on-20240104">Training on <code>20240104</code></h2>
<div id="fig-train-bboxsep" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-train-bboxsep-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="https://i.ibb.co/R7xQbB3/phase5-wandb-bboxsep-visual-filtering.png" class="img-fluid figure-img column-body-outset">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-train-bboxsep-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Training and validation <a href="https://wandb.ai/andrewlook/sketchrnn-pytorch/reports/epoch20240104-bboxsep-max-seq-length-250--Vmlldzo2OTQ1NTgz">loss metrics</a> from models trained on <code>20240104</code> using visual filtering on the bounding-box separated drawings, with maxiumum sequence lengths of 200 (gray) and 250 (blue).
</figcaption>
</figure>
</div>
<p>After training on <code>20240104</code>, the validation losses (gray and blue lines in Figure&nbsp;2) seemed substantially lower than the validation losses from the models trained on the previous dataset (beige, light green).</p>
</section>
<section id="overly-complex-drawings" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="overly-complex-drawings">Overly Complex Drawings</h2>
<p>One failure mode I noticed in the results generated after <a href="http://localhost:7592/blog/posts/slml_part5.html#training-after-bounding-boxes">training</a> on the bounding-box separated dataset <code>20231214-filtered</code> was that some generated drawings had knotted, gnarled lines as in Figure&nbsp;3.</p>
<div id="fig-longdrawings" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-longdrawings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/hycJZ71/phase3-runid-093xwcex-epoch-00900-sample-0035-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/KrrH350/phase3-runid-093xwcex-epoch-01000-sample-0095-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/5jWFN7T/phase3-runid-dhzx8671-epoch-00200-sample-0322-decoded.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-longdrawings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Generated examples with too much complexity.
</figcaption>
</figure>
</div>
<p>Reviewing the <a href="./slml_part5.html#filtering-by-number-of-points">bounding box-separated dataset</a> I noticed that some drawings were of one figure, and some drawings were patterned or chained with many faces.</p>
<div id="fig-over300" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-over300-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-over300" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-1093" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-1093-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/bdXtjzw/16strokes-1093points.png" class="img-fluid figure-img" data-ref-parent="fig-over300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-1093-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) 1093 points
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-over300" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-1127" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-1127-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/S7j9ktg/16strokes-1127points.png" class="img-fluid figure-img" data-ref-parent="fig-over300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-1127-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) 1127 points
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-over300" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-329" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-329-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/6vYXZmt/4strokes-329points.png" class="img-fluid figure-img" data-ref-parent="fig-over300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-329-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) 329 points
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-over300-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Drawings with over 300 points.
</figcaption>
</figure>
</div>
<p>Sometimes I make patterns by chaining repeating sequences of faces into long continuous lines. I wondered whether the presence of this kind of drawing in the training data was occasionally encouraging the model to make long continuous chains rather than drawing a single person.</p>
<div id="fig-chains" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-chains-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="https://i.ibb.co/wYmmGHX/sb42p006-raw-rotated.jpg" class="img-fluid figure-img column-body"></p>
<figcaption>chains example</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chains-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Example of a “diagonal chain” single-line pattern I draw.
</figcaption>
</figure>
</div>
<p>I wanted to exclude those patterns/chains from my training data, so I could give my model the best chance of learning to draw one person at a time.</p>
</section>
<section id="similarity-filtered-dataset" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="similarity-filtered-dataset">Similarity-Filtered Dataset</h2>
<p>I decided to make a subset <code>20240104-furtherfiltered</code> of dataset <code>20240104</code>.</p>
<p>My plan was to compute embeddings for every bounding-box separated drawing in dataset <code>20240104</code>. Then I could K-Means clustering on them, and decide which clusters I wanted to exclude in bulk. <sup>2</sup></p>
<p>Right away I spotted the “too complex” chained line drawings in cluster 0 (Figure&nbsp;6 (a)). There were also several chained line drawings in cluster 3 (Figure&nbsp;6 (b)) mixed in with some squarish horizontal drawings that I was happy to exclude from my training set, as they looked too different from my more typical standalone drawings of individual faces/people.</p>
<div id="fig-clusters-too-complex" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clusters-too-complex-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-too-complex" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-c0" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c0-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/ZghFLD3/c0.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-too-complex">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c0-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Cluster 0
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-too-complex" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-c3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/hHgwZff/c3.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-too-complex">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Cluster 3
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clusters-too-complex-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Clusters with drawings that were “too complex”.
</figcaption>
</figure>
</div>
<p>I also noticed some clusters with drawings that were “too simple”. It seems like many of the drawings in cluster 13 (Figure&nbsp;7 (a)) were stray lines accidentally separated from any context by the bounding-box preprocessing. Cluster 9 (Figure&nbsp;7 (b)) had many similar nonsensical lines, though they were mixed in with some false positives - valid drawings that I’d prefer to keep in the dataset.</p>
<div id="fig-clusters-too-simple" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clusters-too-simple-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-too-simple" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-c13" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c13-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/mBnwc2N/c13.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-too-simple">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c13-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Cluster 13
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-too-simple" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-c9" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/7WskmQ2/c9.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-too-simple">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Cluster 9
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clusters-too-simple-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Clusters with drawings that were “too simple”.
</figcaption>
</figure>
</div>
<div class="page-columns page-full"><p>I was excited to notice some distinct categories in my drawings, seeing them from a distance. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">In the future, as I add more drawings, it’d be great to explicitly label these drawing categories and even train separate models on them. For now, given that I don’t have enough drawings scanned yet, I’m choosing to keep them in one dataset.</span></div></div>
<p>Clusters 1, 4, and 11 (in Figure&nbsp;8 (a), Figure&nbsp;8 (c), and Figure&nbsp;8 (i), respectively) all have vertical, narrow, whole-body figures.</p>
<p>Cluster 2, in Figure&nbsp;8 (b), mostly has rounder compositions of individual faces without a complete body.</p>
<p>Clusters 8 and 15, in Figure&nbsp;8 (g) and Figure&nbsp;8 (l), seem to have more complex drawings but mostly still contain drawings of standalone people.</p>
<p>The remaining clusters contain reasonably uniform drawings of standalone people, in vertical compositions, that are not too narrow. Hovering your mouse over these links Figure&nbsp;8 (d), Figure&nbsp;8 (e), Figure&nbsp;8 (f), Figure&nbsp;7 (b), Figure&nbsp;8 (h), Figure&nbsp;8 (j), Figure&nbsp;8 (k).</p>
<div id="fig-clusters-good" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clusters-good-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-good" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-c1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/fnYzJBP/c1.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-good">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Cluster 1
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-good" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-c2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/fQXBkPs/c2.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-good">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Cluster 2
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-good" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-c4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/nm51Z52/c4.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-good">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Cluster 4
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-good" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-c5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/3MsTpmc/c5.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-good">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d) Cluster 5
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-good" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-c6" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/0D5zrnD/c6.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-good">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(e) Cluster 6
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-good" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-c7" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/vHYw3K7/c7.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-good">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(f) Cluster 7
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-good" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-c8" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/ZGdC3fV/c8.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-good">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(g) Cluster 8
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-good" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-c10" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/cJp9zLP/c10.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-good">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(h) Cluster 10
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-good" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-c11" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/kDtRfnL/c11.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-good">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(i) Cluster 11
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-good" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-c12" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/R7pRzjc/c12.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-good">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(j) Cluster 12
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-good" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-c14" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c14-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/RpWdZMp/c14.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-good">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c14-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(k) Cluster 14
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-clusters-good" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-c15" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-c15-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/dKPYGQk/c15.png" class="img-fluid figure-img" data-ref-parent="fig-clusters-good">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-c15-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(l) Cluster 15
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clusters-good-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Clusters with drawings that looked good to me.
</figcaption>
</figure>
</div>
</section>
<section id="training-on-filtered-dataset" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="training-on-filtered-dataset">Training on Filtered Dataset</h2>
<p>The remainder of the clusters, in Figure&nbsp;8, looked “good enough” for me to include in my training set. I all other clusters, and saved a filtered-down dataset as <code>20240104-furtherfiltered</code>.</p>
<p>Compared to dataset <code>20240104</code>, it’s clear in the top row of Figure&nbsp;9 that in the filtered dataset variant, the distribution of number of strokes has shifted away from the long tail of many-stroke drawings.</p>
<div id="fig-dataset-20240104" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-dataset-20240104-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="https://i.ibb.co/b6dV2Sf/dataset-comparison-2024014-furtherfiltered.png" class="img-fluid figure-img column-body">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dataset-20240104-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Comparing to unfiltered dataset <code>20240104</code> (2100 drawings) to the filtered dataset <code>20240104-furtherfiltered</code> (1300 drawings).
</figcaption>
</figure>
</div>
<p>Comparing the training metrics in Figure&nbsp;10 for the model trained on filtered dataset <code>20240104-furtherfiltered</code> (in red) with the previous model runs on unfiltered dataset <code>20240104</code> (in gray and blue) is not a perfect comparison. Since the validation set for <code>20240104-furtherfiltered</code> was also filtered, it’s a smaller (and likely noisier) validation set. Still, the new model’s validation loss was roughly within the bounds of what I expected.</p>
<div id="fig-train-bboxsep-furtherfiltered" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-train-bboxsep-furtherfiltered-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="https://i.ibb.co/NpFdRRB/phase5-wandb-bboxsep-visual-filtering-with-furtherfiltered.png" class="img-fluid figure-img column-body-outset">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-train-bboxsep-furtherfiltered-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Training and validation <a href="https://wandb.ai/andrewlook/sketchrnn-pytorch/reports/epoch20240104-visual-filtering-bboxsep--Vmlldzo3MTMwMjAz">loss metrics</a> from models trained on <code>20240104-furtherfiltered</code> using visual filtering on the bounding-box separated drawings (red).
</figcaption>
</figure>
</div>
<!-- ![](https://i.ibb.co/42FGqDN/phase5-wandb-bboxsep-visual-filtering-with-furtherfiltered.png)-->
<p>Qualitatively, the generated results after visual similarity filtering were significantly improved.</p>
<div id="fig-gen-bboxsep" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gen-bboxsep-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/85p1XNk/phase5-test.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/R41PJTS/phase5-test2.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/SymDvdW/phase5-test3.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gen-bboxsep-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Generated samples after training with visual filtering on bbox-separated dataset.
</figcaption>
</figure>
</div>
<p>Even the generated results that looked less like people/faces to me still had appealing curves and flowing patterns, which I recognize from my own drawing style.</p>
<div id="fig-gen-bboxsep-oddones" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gen-bboxsep-oddones-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://i.ibb.co/xqfCt4n/phase5-test4.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://i.ibb.co/cbh6D3F/phase5-test5.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gen-bboxsep-oddones-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Generated samples after training with visual filtering on bbox-separated dataset.
</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p><em>If you want to keep reading, check out <a href="../../blog/posts/slml_part7.html">part 7</a> of my <a href="../../projects/slml.html">SLML</a> series.</em></p>
</blockquote>
<!--
=== Phase 5 - BBoxsep+Filtering ===

Jan 13:
- gc0el8ta: [fallen-microwave-32\_\_v10-epoch20240104\_bboxsep-filtering | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/gc0el8ta?workspace=user-andrewlook)
    - Dataset: epoch20240104_trainval09
- [ ] 24mzu9rc: [bright-sea-33\_v11-maxseqlen-250 | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/24mzu9rc?workspace=user-andrewlook)
    - Dataset: epoch20240104_trainval09
    - max_seq_length: 250 (instead of 200)
- w4m3rxgi: [atomic-tree-34\_futherfiltered | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/w4m3rxgi/overview?workspace=user-andrewlook)
    - Dataset: epoch20240104_furtherfiltered_trainval09

-->


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>based on what I observed when <a href="../../blog/posts/slml_part5.html">filtering the bounding-box dataset</a> by number of points↩︎</p></li>
<li id="fn2"><p>similar to what I did with full sketchbook pages in <a href="../../blog/posts/slml_part1.html">part 1</a>↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>singleline</category>
  <category>machinelearning</category>
  <category>slml</category>
  <guid>https://andrewlook.com/notebooks/blog/posts/slml_part6.html</guid>
  <pubDate>Thu, 04 Jan 2024 00:00:00 GMT</pubDate>
  <media:content url="https://i.ibb.co/BPdSrTQ/phase1-wandb-with-without-minn10.png" medium="image" type="image/png"/>
</item>
<item>
  <title>SLML Part 5 - SketchRNN Experiments: Path Preprocessing</title>
  <dc:creator>Andrew Look</dc:creator>
  <link>https://andrewlook.com/notebooks/blog/posts/slml_part5.html</link>
  <description><![CDATA[ 




<section id="slml-part-5---sketchrnn-experiments-path-preprocessing" class="level1 page-columns page-full">
<h1>SLML Part 5 - SketchRNN Experiments: Path Preprocessing</h1>
<blockquote class="blockquote">
<p><em>This post is part 5 of <a href="../../projects/slml.html">“SLML” - Single-Line Machine Learning</a>.</em></p>
<p><em>To read the previous post, check out <a href="../../blog/posts/slml_part4.html">part 4</a>. If you want to keep reading, check out <a href="../../blog/posts/slml_part6.html">part 6</a>.</em></p>
</blockquote>
<section id="path-joining" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="path-joining">Path Joining</h2>
<p>Despite some improvement within the strokes, my latest models were still producing many short strokes. So I figured that improving my dataset would give better results than fiddling with hyper-parameters or my model.</p>
<p>Looking at my training data, I returned to an issue from the stroke3-conversion process: autotrace hadn’t realized that the different segments of my lines were actually connected. It was producing a series of small, centerline-traced segments, as seen in Figure&nbsp;1 (a).</p>
<p>I’d need some preprocessing to connect the smaller strokes into larger strokes, so that my SketchRNN model would learn to generate long contiguous single-line drawings.</p>
<div id="fig-joining" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-joining-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-joining" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-original" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-original-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/s21gLTQ/stroke3-01-sepstrokes.png" class="img-fluid figure-img" data-ref-parent="fig-joining">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-original-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Original: 22 strokes
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-joining" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-joined" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-joined-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/602dKpr/stroke3-01-joinedstrokes.png" class="img-fluid figure-img" data-ref-parent="fig-joining">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-joined-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) After path-joining: 4 strokes
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-joining" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-spliced" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-spliced-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/rtnZMq1/stroke3-01-splicedstrokes1.png" class="img-fluid figure-img" data-ref-parent="fig-joining">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-spliced-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) After path-splicing: 2 strokes
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-joining-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Comparison of number of strokes before and after preprocessing, with one color per stroke.
</figcaption>
</figure>
</div>
<p>My first preprocesing improvement was simple algorithm that I called <a href="https://singleline-dataset.andrewlook.com/strokes.html#joining-paths">“path joining”</a>.</p>
<div class="page-columns page-full"><p>Each of my drawings is represented as a collection of line segments, each with a start and end point. I try to find the shortest connection I can draw to connect two segments. Starting with the longest stroke, I compare it each of the other strokes and calculate the minimum distance between the start and end points of the line segments. After going through all the strokes in a drawing, I connect the two strokes with the shortest gap between their endpoints. I repeat this process, joining the strokes until no strokes remain with endpoints more than 30 pixels apart. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">I set an upper bound on the path-joining distance of 30 pixels (in a 200x200 pixel image) was the maximum distance I observed made sense before I started to erroneously join strokes that were too far apart.</span></div></div>
<p>Though path-joining improved the average stroke length as seen in Figure&nbsp;1 (b), I noticed some drawings had large contiguous strokes that weren’t getting connected. I realized that while the strokes were close together and in some cases touching, their start and end points were far apart from each other.</p>
</section>
<section id="path-splicing" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="path-splicing">Path Splicing</h2>
<p>My next preprocessing improvement, <a href="https://singleline-dataset.andrewlook.com/strokes.html#splicing-strokes">“path splicing”</a>, would run after “path joining” and attempt to address this problem.</p>
<p>After path joining leaves a smaller number of strokes, I want to find the shortest connections to combine multiple longer strokes. Starting with the longest path, I look for a smaller stroke that I could “splice” into the middle of the larger stroke. For each candidate stroke, I’d step through each point on the larger stroke and compare its distance from the start and end points of the shorter paths. When I found the smallest gap, I would “splice” the shorter line into the longer path at the point with the smallest distance.</p>
<p>While not every drawing was turned into a single stroke, this was a big improvement, as seen in Figure&nbsp;1 (c).</p>
<div id="fig-compare-i16" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-compare-i16-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="https://i.ibb.co/d6B6JQT/dataset-comparison-minn10-vs-splicedata.png" class="img-fluid figure-img column-body">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compare-i16-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <a href="../../blog/posts/slml_part4.html#filtering-out-short-strokes">Previous</a> dataset <code>look_i16__minn10</code>, left, compared to path-spliced dataset <code>v2-splicedata</code>, right.
</figcaption>
</figure>
</div>
<p>Based my earlier experiment showing the benefits of <a href="./slml_part4.html#filtering-out-short-strokes">short-stroke exclusion</a>, I wanted to try training a model on this new dataset.</p>
</section>
<section id="training-after-path-splicing" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="training-after-path-splicing">Training after Path-Splicing</h2>
<p>I ran the preprocessing on the whole dataset. Next, I filtered the original 1300 drawings to exclude any drawings with more than 6 strokes, resulting in a new dataset of 1200 drawings that I named <code>v2-splicedata</code>. Then I trained a new set of models, keeping the layernorm and recurrent dropout enabled.</p>
<div id="fig-train-splice" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-train-splice-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="https://i.ibb.co/vDsPsK6/phase3-wandb-splicedata.png" class="img-fluid figure-img column-page">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-train-splice-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Training and validation <a href="https://wandb.ai/andrewlook/sketchrnn-pytorch/reports/v2-splicedata-vs-look_i16_minn10--Vmlldzo2OTQ1NTIw">loss metrics</a> of recurrent dropout model (light green) alongside models trained on this joined/spliced dataset (turquoise/dark green).
</figcaption>
</figure>
</div>
<p>After training some models on a path-spliced dataset the training metrics aren’t a perfect comparison, since the content of the validation also changed when I applied path-splicing. Still, I can see from the validation loss graph that the model started to overfit around 1000 steps. The roughly similar shapes of the early train and validation loss curves at least convinced me that the model hadn’t gotten dramatically worse.</p>
<div id="fig-gen-spliced" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gen-spliced-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/ZGLMZ4H/phase3-runid-dhzx8671-epoch-00200-sample-0338-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/5TVZG6H/phase3-runid-dhzx8671-epoch-00400-sample-0338-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/KxRj62q/phase3-runid-093xwcex-epoch-01000-sample-0061-decoded.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gen-spliced-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Generated samples from a model trained on path-spliced dataset.
</figcaption>
</figure>
</div>
<p>Qualitatively, the generated drawings showed a big improvement. The model had learned to generate longer unbroken strokes. I started to notice results that looked more like single-line drawings of people. In some cases they look surreal, but I started to see some more recognizable face elements and in some cases full bodies.</p>
<div id="fig-debug-splicing" class="column-body-outset quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-debug-splicing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="https://storage.googleapis.com/andrewlook-art-assets/andrewlook.com/videos/new_file_fps20.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-debug-splicing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Iterating through full dataset, with 3 frames per drawing: original, path-joined, and path-spliced. I liked the slight jitter in the animation as I watched the drawings go from colorful (many strokes) to fully blue (single stroke).
</figcaption>
</figure>
</div>
</section>
<section id="bounding-boxes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="bounding-boxes">Bounding Boxes</h2>
<p>My last remaining problem: there were some pages where I’d make 4 or 5 separate drawings that had nothing to do with each other, and had a lot of space between them.</p>
<p>I wanted to separate those into separate examples before making a training dataset, for 2 reasons:</p>
<ol type="1">
<li><strong>To get more training examples</strong> from my limited number of scanned drawings.</li>
<li><strong>To avoid confusing the model</strong> when some examples are complex drawings with multiple people, and other drawings are just one person. </li>
</ol>
<div class="no-row-height column-margin column-container"><span class="margin-aside">If the model sometimes learngs to make a complete drawing and then start a second unrelated drawing, how does the model know when to finish a drawing vs.&nbsp;when to start a second drawing alongside the first one?</span></div><p>My intuition for my third preprocessing improvement, <a href="https://singleline-dataset.andrewlook.com/bounding_boxes.html">“bounding box separation”</a>, came when I noticed how unrelated strokes within a drawing didn’t overlap much, and tended to be spaced far apart. For each of the stroke within a drawing, I’d determine its top/bottom/left/right extremes and consider a box around each stroke as in Figure&nbsp;6.</p>
<div id="fig-bbox-0" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bbox-0-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://i.ibb.co/TRWchdV/bbox-0.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://i.ibb.co/g3BZnn8/bbox-1.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bbox-0-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Example drawing with multiple unrelated strokes. <!--
Resulting training records:
![bbox-10](https://i.ibb.co/WWXtxmx/bbox-10.png)
![bbox-11](https://i.ibb.co/3MLYQGs/bbox-11.png)
![bbox-12](https://i.ibb.co/1R31PNF/bbox-12.png)
![bbox-13](https://i.ibb.co/hHcFg6t/bbox-13.png)
-->
</figcaption>
</figure>
</div>
<p>Then for each combination of bounding boxes, I’d compute a ratio of the area of their overlap compared to the area of the non-overlapping parts as in Figure&nbsp;7.</p>

<div class="no-row-height column-margin column-container"><div id="fig-iou" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iou-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/txyxw7z/bbox-4-IOU.webp" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iou-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Intersection over Union (“IOU”) Metric.
</figcaption>
</figure>
</div></div><p>If the ratio exceeds some threshold, I consider them to be part of the same drawing, and I merge the bounding boxes as in Figure&nbsp;9 (a). Combining that merged bounding box along with all the remaining bounding boxes, I repeat the process until no bounding-box intersections remain that exceed the threshold.</p>
<div id="fig-bbox-1" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bbox-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.ibb.co/gV1MFMZ/bbox-3.png" class="img-fluid figure-img"></p>
<figcaption>IOU = 0.03</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.ibb.co/PxDD7t6/bbox-2.png" class="img-fluid figure-img"></p>
<figcaption>IOU = 0.12</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bbox-1" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-merged" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-merged-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/x5gYgB1/bbox-5-merge.png" class="img-fluid figure-img" data-ref-parent="fig-bbox-1">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-merged-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Merged BBoxes
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bbox-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Comparison of high-IOU vs.&nbsp;low-IOU bounding box intersections.
</figcaption>
</figure>
</div>
<p>Also, if any bounding boxes have a really small area, I just drop them. It turns out this helps exclude small scribbles of text that were ending up in my training data as separate strokes - for example, the page number at the bottom right of Figure&nbsp;9 (a).</p>
<div id="fig-bbox-1" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bbox-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.ibb.co/bQM69Jk/bbox-7.png" class="img-fluid figure-img"></p>
<figcaption>Original: 4 Strokes</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bbox-1" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-pagenum" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pagenum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/yN4Ct0g/bbox-9.png" class="img-fluid figure-img" data-ref-parent="fig-bbox-1">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pagenum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Tiny page number: 1 stroke
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.ibb.co/hDhs4Nz/bbox-6.png" class="img-fluid figure-img"></p>
<figcaption>Merge: 3 Strokes</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.ibb.co/93fz1zK/bbox-8.png" class="img-fluid figure-img"></p>
<figcaption>Result</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bbox-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Example from training set of a very small stroke being removed.
</figcaption>
</figure>
</div>
</section>
<section id="dataset-20231214" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="dataset-20231214">Dataset <code>20231214</code></h2>
<p>Once I have all the separated strokes, I save then into a new dataset as separate drawings. While the previous dataset <code>v2-splicedata</code> only has 1200 drawings, the new bounding-box separated dataset <code>20231214</code> has 2400 drawings.</p>
<div id="fig-compare-datasets" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-compare-datasets-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="https://i.ibb.co/vvMWrDc/dataset-comparison-with-v2-splicedata.png" class="img-fluid figure-img column-body-outset">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compare-datasets-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Comparison of previous dataset <code>v2-splicedata</code> with <code>20231214</code> and <code>20231214-filtered</code>.
</figcaption>
</figure>
</div>
<p>The dataset grew from 1200 to 2400 drawings because pages containing multiple distinct drawings (such as Figure&nbsp;11 (a)) were divided into separate rows in the new training set (like Figure&nbsp;11 (b), Figure&nbsp;11 (c), Figure&nbsp;11 (d)).</p>
<div id="fig-bbox-split-example" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bbox-split-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bbox-split-example" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-prebbox" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-prebbox-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/SrpsMWZ/bbox-20.png" class="img-fluid figure-img" data-ref-parent="fig-bbox-split-example">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-prebbox-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Original
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bbox-split-example" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-postbbox-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-postbbox-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/PNLqQLG/bbox-21.png" id="fig-postbbox-1" class="img-fluid figure-img" data-ref-parent="fig-bbox-split-example">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-postbbox-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b)
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bbox-split-example" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-postbbox-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-postbbox-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/wWKK6zC/bbox-22.png" id="fig-postbbox-2" class="img-fluid figure-img" data-ref-parent="fig-bbox-split-example">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-postbbox-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c)
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bbox-split-example" style="flex-basis: 25.0%;justify-content: flex-start;">
<div id="fig-postbbox-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-postbbox-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/N18YMC0/bbox-23.png" id="fig-postbbox-3" class="img-fluid figure-img" data-ref-parent="fig-bbox-split-example">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-postbbox-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d)
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bbox-split-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Original drawing (left) was one row in dataset <code>v2-splicedata</code>. The rightmost three drawings are distinct rows in dataset <code>20231214</code>.
</figcaption>
</figure>
</div>
<p>The new separated drawings looked more visually consistent with the average drawing out of the training set as a whole. The new dataset contains far more single-character drawings, so I expect that the RNN will benefit on learning from a set of drawings with more similar subject matter.</p>
<p>I hypothesized that the bbox-separated dataset will be a big improvement because of the consistency of the resulting drawings. Before, the model was learning that sometimes drawings end after one person is drawn, but sometimes we move the pen and start a new person.</p>
</section>
<section id="filtering-by-number-of-points" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="filtering-by-number-of-points">Filtering by Number of Points</h2>
<p>Looking at the distribution of number of points per drawing in the new dataset <code>20231214</code> in the bottom-middle chart in Figure&nbsp;10, I noticed a long tail of drawings with more than 500 points. To explore this, I created a variant dataset <code>20231214-filtered</code>.</p>
<p>Dataset <code>20231214-filtered</code> which was filtered down to 1800 drawings, keeping only drawings with more than 50 and less than 300 points as you can see in the bottom-right chart in Figure&nbsp;10.</p>
<div class="page-columns page-full"><p>Wondering if drawings with many points were less likely to have consistent subject matter (individual people) than the rest of the training set, I sampled some drawings with over 300 points. While drawings such as Figure&nbsp;12 (a) and Figure&nbsp;12 (b) were obvious candidates to exlcude, there were valid drawings near the margin such as Figure&nbsp;12 (c) that I would be excluding after I picked a threshold.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Possible Improvement: Filtering by visual embedding might be more reliable to exclude complex drawings</span></div></div>
<div id="fig-over300" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-over300-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-over300" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-1093" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-1093-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/bdXtjzw/16strokes-1093points.png" class="img-fluid figure-img" data-ref-parent="fig-over300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-1093-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) 1093 points
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-over300" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-1127" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-1127-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/S7j9ktg/16strokes-1127points.png" class="img-fluid figure-img" data-ref-parent="fig-over300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-1127-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) 1127 points
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-over300" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-329" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-329-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/6vYXZmt/4strokes-329points.png" class="img-fluid figure-img" data-ref-parent="fig-over300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-329-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) 329 points
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-over300-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Drawings with over 300 points.
</figcaption>
</figure>
</div>
<div class="page-columns page-full"><p>I also looked at the low end of the distribution and found drawings with under 50 points. There were nonsensical squiggles such as Figure&nbsp;13 (a) that I was happy to exclude. There were cases below the 50 point threshold such as Figure&nbsp;13 (b) and Figure&nbsp;13 (c) that looked recognizable as my drawings, but had been simplified by RDP too aggressively. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">Possible Improvement: Applying RDP after bounding box separation rather than before.</span></div></div>
<div id="fig-under50" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-under50-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-under50" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-21" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-21-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/CPRMhHr/21points.png" class="img-fluid figure-img" data-ref-parent="fig-under50">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-21-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) 21 points
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-under50" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-30" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-30-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/2Pw6zvG/30points.png" class="img-fluid figure-img" data-ref-parent="fig-under50">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-30-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) 30 points
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-under50" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-41" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-41-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/VJP5LY4/41points.png" class="img-fluid figure-img" data-ref-parent="fig-under50">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-41-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) 41 points
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-under50-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Drawings with under 50 points.
</figcaption>
</figure>
</div>
</section>
<section id="training-after-bounding-boxes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="training-after-bounding-boxes">Training after Bounding Boxes</h2>
<div id="fig-train-bbox" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-train-bbox-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="https://i.ibb.co/729d0KL/phase4-wandb-bboxsep-filtering.png" class="img-fluid figure-img column-page">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-train-bbox-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Training and validation <a href="https://wandb.ai/andrewlook/sketchrnn-pytorch/reports/epoch20231214-bounding-box-separation-filtering--Vmlldzo2OTQ1NTUw">loss metrics</a> comparing model trained on <code>20231214</code> (beige) compared with models trained on dataset <code>20231214-filtered</code> with and without stroke augmentation (green, burgundy).
</figcaption>
</figure>
</div>
<p>After training on the unfiltered dataset <code>20231214</code>, I noticed that some drawings were devolving into a sequence of repeated face features without forming a cohesive person or face, as in Figure&nbsp;15.</p>
<div id="fig-gen-unfiltered" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gen-unfiltered-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/CV2SXQ3/phase4-runid-g3hupc1l-epoch-00300-sample-0136-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/x51j8wR/phase4-runid-g3hupc1l-epoch-00200-sample-0180-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/vdrzxMj/phase4-runid-g3hupc1l-epoch-00400-sample-0136-decoded.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gen-unfiltered-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Generated results after training on unfiltered dataset <code>20231214</code>.
</figcaption>
</figure>
</div>
<!--
![](https://i.ibb.co/6vCfK1D/phase4-runid-g3hupc1l-epoch-00400-sample-0207-decoded.png)
![](https://i.ibb.co/0mxFh2X/phase4-runid-g3hupc1l-epoch-00400-sample-0396-decoded.png)
-->
<p>The model results in Figure&nbsp;16 after training on filtered dataset <code>20231214-filtered</code> appear qualitatively better to me. The best results I could find had long coherent strokes capturing part of a face and sometimes a corresponding body.</p>
<div id="fig-gen-bbox1" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gen-bbox1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/wh86Rm5/phase4-runid-fj4klg1i-epoch-00200-sample-0080-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/R28gn3K/phase4-runid-fj4klg1i-epoch-00200-sample-0291-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/V2MdNGf/phase4-runid-fj4klg1i-epoch-00400-sample-0186-decoded.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gen-bbox1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Generated results after training on filtered dataset <code>20231214-filtered</code>.
</figcaption>
</figure>
</div>
<!--
![](https://i.ibb.co/d2vfKZ2/phase4-runid-fj4klg1i-epoch-00100-sample-0080-decoded.png)
![](https://i.ibb.co/xYWW05f/phase4-runid-fj4klg1i-epoch-00100-sample-0278-decoded.png)
![](https://i.ibb.co/WkYTZxh/phase4-runid-fj4klg1i-epoch-00400-sample-0278-decoded.png)
-->
<p>The model results in Figure&nbsp;17 after training with stroke-augmentation on filtered dataset <code>20231214-filtered</code> appear to be roughly of similar quality to . The best results I could find had long coherent strokes capturing part of a face and sometimes a corresponding body.</p>
<div id="fig-gen-bbox2" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gen-bbox2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/nmLtzS7/phase4-runid-slgleu3l-epoch-00100-sample-0244-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/4Mq5Gtw/phase4-runid-slgleu3l-epoch-00100-sample-0068-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/vXsBtDS/phase4-runid-slgleu3l-epoch-00100-sample-0406-decoded.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gen-bbox2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Generated results after training on filtered dataset <code>20231214-filtered</code>, with stroke augmentation enabled.
</figcaption>
</figure>
</div>
<!--
![](https://i.ibb.co/Fmf7RBF/phase4-runid-slgleu3l-epoch-00100-sample-0268-decoded.png)
-->
<!--
TODO: takeaways

- improving dataset > improving hyperparams
- preprocessing worth the investment
- consider filtering by embeddings of bbox-sep'd drawings
- apply RDP late in the process.
-->
<blockquote class="blockquote">
<p><em>If you want to keep reading, check out <a href="../../blog/posts/slml_part6.html">part 6</a> of my <a href="../../projects/slml.html">SLML</a> series.</em></p>
</blockquote>
<!--

=== Phase 3 ===

Dec 7:
- [ ] 093xwcex: [likely-glitter-26\_splicedata-maxstrokes5 | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/093xwcex?workspace=user-andrewlook)
    - Dataset: v2-splice-maxstrokes5
- [ ] dhzx8671: [pretty-firebrand-27\_splicedata-maxstrokes6 | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/dhzx8671?workspace=user-andrewlook)
    - Dataset: v2-splice-maxstrokes6

=== Phase 4 ===

Dec 17,18:
- [ ] gh3hupc1l: [treasured-serenity-28\_\_epoch-20231214\_\_boundingboxes | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/g3hupc1l?workspace=user-andrewlook)
    - Dataset: epoch-20231214-trainval
- [ ] fj4klg1i: [eager-capybara-29 | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/fj4klg1i?workspace=user-andrewlook)
    - Dataset: epoch-20231214-filtered-trainval
- [ ] slgleu3l: [summer-planet-30 | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/slgleu3l?workspace=user-andrewlook)
    - Dataset: epoch-20231214-filtered-trainval
    - augment_stroke_prob=1
- [ ] wz9rw5a4: [comfy-violet-31 | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/wz9rw5a4/overview?workspace=user-andrewlook)
    - use_random_scale=true
    - random_scale_factor=0.15

-->


</section>
</section>

 ]]></description>
  <category>singleline</category>
  <category>machinelearning</category>
  <category>slml</category>
  <guid>https://andrewlook.com/notebooks/blog/posts/slml_part5.html</guid>
  <pubDate>Thu, 14 Dec 2023 00:00:00 GMT</pubDate>
  <media:content url="https://i.ibb.co/BPdSrTQ/phase1-wandb-with-without-minn10.png" medium="image" type="image/png"/>
</item>
<item>
  <title>SLML Part 4 - SketchRNN Experiments: Minimum Stroke Length and RNN Regularization</title>
  <dc:creator>Andrew Look</dc:creator>
  <link>https://andrewlook.com/notebooks/blog/posts/slml_part4.html</link>
  <description><![CDATA[ 




<section id="slml-part-4---sketchrnn-experiments-minimum-stroke-length-and-rnn-regularization" class="level1 page-columns page-full">
<h1>SLML Part 4 - SketchRNN Experiments: Minimum Stroke Length and RNN Regularization</h1>
<blockquote class="blockquote">
<p><em>This post is part 4 of <a href="../../projects/slml.html">“SLML” - Single-Line Machine Learning</a>.</em></p>
<p><em>To read the previous post, check out <a href="../../blog/posts/slml_part3.html">part 3</a>. If you want to keep reading, check out <a href="../../blog/posts/slml_part5.html">part 5</a>.</em></p>
</blockquote>
<p>Now that I had a repeatable process to convert JPEGs into stroke-3, I decided to start training models with my first dataset (which I called <code>look_i16</code>).</p>
<section id="filtering-out-short-strokes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="filtering-out-short-strokes">Filtering out Short Strokes</h2>
<p>The training data in my first dataset, <code>look_i16</code>, had lots of tiny strokes mixed in with longer ones, and no inherent order to them.</p>
<div id="fig-i16-samples" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-i16-samples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://i.ibb.co/CB9BQTY/phase1-sample-0167-epoch-01700-orig.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://i.ibb.co/0JBLg6Q/phase1-sample-0177-epoch-01700-orig.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-i16-samples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Samples from dataset <code>look_i16</code>.
</figcaption>
</figure>
</div>
<p>Unsurprisingly, my first models produced odd smatterings composed of many small strokes.</p>
<div id="fig-i16-results" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-i16-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/Nnrn9jL/phase1-sample-0111-epoch-00330-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/fSnW50p/phase1-sample-0115-epoch-00310-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/LNdGhBb/phase1-sample-0177-epoch-06000-decoded.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-i16-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Generated outputs from SketchRNN trained on dataset <code>look_i16</code>.
</figcaption>
</figure>
</div>
<div class="page-columns page-full"><p>As a quick experiment, I tried just filtering out any really short strokes out of the dataset - I decided to iterate through each of the 1300 drawings in the training set, and filter out any strokes with less than 10 points - I called this dataset <code>look_i16__minn10</code>. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">I recognized that I’d need a more systematic solution to improve my training data, but I wanted to try a quick improvement before going deeper. See <a href="../../blog/posts/slml_part5.html">part 5</a> for how I updated my preprocessing to reduce the number of strokes in my training data.</span></div></div>
<div id="fig-compare-i16" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-compare-i16-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/5RbWt2p/dataset-comparison-i16.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compare-i16-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Distribution of num_strokes and num_points in datasets <code>look_i16</code> and <code>look_i16__minn10</code>. Both datasets have 1300 entries in the training set.
</figcaption>
</figure>
</div>
<p>Note that the average number of strokes in <code>look_i16</code> is around 40, while it’s closer to 10 in <code>look_i16__minn10</code>. It seems that there were many very short strokes in the training data. I also <a href="http://localhost:4954/blog/posts/slml_part3.html#line-simplification">simplified the drawings</a> more aggressively for <code>look_i16__minn10</code> by increasing RDP’s <code>epsilon</code> parameter to <code>1.0</code> when preprocessing the data, which further reduced the number of points per drawing.</p>
<div id="fig-phase1-training" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-phase1-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="https://i.ibb.co/BPdSrTQ/phase1-wandb-with-without-minn10.png" class="img-fluid figure-img column-page">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-phase1-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Training and validation <a href="https://wandb.ai/andrewlook/sketchrnn-pytorch/reports/look_i16-vs-look_i16_minn10_epsilon1--Vmlldzo2OTQ1NDgz">loss metrics</a> for <code>look_i16</code> (pink) and <code>look_i16__minn10</code> (yellow).
</figcaption>
</figure>
</div>
<p>When training models, I’m measuring the reconstruction loss. I feed in a drawing, the model encodes it into an embedding vector, and then decodes the embedding vector back into a drawing. I can compute the loss at each step in the reconstructed drawing compare to the original. Periodically, after processing several batches of training data, I compute the reconstruction metrics on a validation set. This is a portion of the dataset I’m not using to actually update the weights of the model during training.</p>
<p>By comparing the reconstruction loss on the training set vs.&nbsp;the validation set over time, I can identify when the model starts “overfitting”. Intuitively, if the model is learning to perform better on the training data while performing worse on the validation data, that means it is effectively memorizing the training set rather than learning to generalize its learnings to drawings it wasn’t trained on.</p>
<p>The model trained on <code>look_i16__minn10</code> performed slightly better than the model trained on <code>look_i16</code> in terms of the error when reconstructing a drawing. It’s visible in Figure&nbsp;4 that the loss values were lower, and the validation loss didn’t start to increase until slightly later.</p>
<div id="fig-i16-results" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-i16-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/hXBK8Zb/phase2-sample-0066-epoch-11700-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/jzgHHPS/phase2-sample-0145-epoch-11500-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/0mwbJBw/phase2-sample-0291-epoch-13200-decoded.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-i16-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Generated outputs from SketchRNN trained on dataset <code>look_i16__minn10</code>.
</figcaption>
</figure>
</div>
<p>The results produced after training on <code>look_i16__minn10</code> were much less chaotic. While they didn’t resemble coherent drawings, this was the first time I spotted some elements of my drawing style (head shape, eye style, lips, chin).</p>
</section>
<section id="layer-normalization" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="layer-normalization">Layer Normalization</h2>
<!-- TODO: explain layer norm -->
<p>The Magenta team had recommended using <a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a> and <a href="https://arxiv.org/abs/1603.05118">Recurrent Dropout with Memory Loss</a>.</p>
<p>I noticed that when I let the trainer run overnight, I’d get wild spikes in the training loss. I decided to start with Layer Normalization.</p>
<div id="fig-training-layernorm" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-training-layernorm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="https://i.ibb.co/9vk4W7d/phase2-wandb-V2-before-after-layernorm.png" class="img-fluid figure-img column-body-outset">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-training-layernorm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Training and validation <a href="https://wandb.ai/andrewlook/sketchrnn-pytorch/reports/look_i16_minn10-before-after-layernorm--Vmlldzo2OTQ1MzIw">loss metrics</a> from training on <code>look_i16__minn10</code> without layernorm (yellow) and with layernorm (burgundy: learnable=True, gray: learnable=False). The yellow line without layernorm enabled is the same as in Figure&nbsp;4, but this graph shows more than 150,000 train steps while the previous graph showed only 7,000 train steps.
</figcaption>
</figure>
</div>
<p>Adding layer normalization showed a dramatic difference, visible in Figure&nbsp;6. The yellow line (without layer norm) has a massive increase in validation loss, and many big spikes, while the burgundy and gray lines (with layer norm integrated) have much lower validation loss and don’t have any comparable spikes.</p>
<div id="fig-generated-layernorm" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-generated-layernorm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/Y3bZmzg/phase2-runid-zjan5lxh-epoch-00200-sample-0124-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/JHc4C9R/phase2-runid-zjan5lxh-epoch-00200-sample-0230-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/fXX5tDX/phase2-runid-zjan5lxh-epoch-00200-sample-0122-decoded.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-generated-layernorm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Generated samples from model trained with Layer Normalization.
</figcaption>
</figure>
</div>
</section>
<section id="recurrent-dropout" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="recurrent-dropout">Recurrent Dropout</h2>
<!-- TODO: explain recurrent dropout -->
<p>The results from the layernorm models in Figure&nbsp;7 had some hints of my drawing style, while still struggling to form coherent drawings.</p>
<p>Next, I kept layernorm enabled and enabled recurrent dropout. I ran one separate runs with and without stroke augmentation enabled.</p>
<div id="fig-recurrentdropout" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-recurrentdropout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="https://i.ibb.co/xJD1VMg/phase2-wandb-V2-recurrent-dropout.png" class="img-fluid figure-img column-body-outset">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-recurrentdropout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Training and validation <a href="https://wandb.ai/andrewlook/sketchrnn-pytorch/reports/look_i16_minn10-layernorm-with-without-recurrent-dropout--Vmlldzo2OTQ1Mzk5">loss metrics</a> for layernorm-only models (burgundy and gray) alongside models trained with recurrent dropout (green) and with stroke augmentation (pink).
</figcaption>
</figure>
</div>
<p>Compared to the layernorm-only models (burgundy and gray), the one recurrent dropout model (green) achieved a lower validation loss relatively quickly, before starting to overfit.</p>
<p>The model trained with recurrent dropout and stroke augmentation (pink) clearly performed worse than with recurrent dropout (in terms of higher validation loss).</p>
<div id="fig-generated-dropout" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-generated-dropout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/f1v0NNP/phase2-runid-cal3jv56-epoch-01100-sample-0199-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/ZYB0NrB/runid-cal3jv56-epoch-01100-sample-0056-decoded.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://i.ibb.co/HzcbHny/runid-cal3jv56-epoch-01100-sample-0243-decoded.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-generated-dropout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Generated samples from model trained with Recurrent Dropout as well as Layer Normalization.
</figcaption>
</figure>
</div>
<p>The resulting generations from model with layernorm and recurrent dropout in Figure&nbsp;9 weren’t obviously better or worse than those from the layernorm-only model in Figure&nbsp;7.</p>
<!--
## TODO: Takeaways
Removing short strokes from the training data made a big difference. Based on the gains from a simple filtering, I'm optimistic that better preprocessing will yield big improvements 
-->
<blockquote class="blockquote">
<p><em>If you want to keep reading, check out <a href="../../blog/posts/slml_part5.html">part 5</a> of my <a href="../../projects/slml.html">SLML</a> series.</em></p>
</blockquote>
<!-- 
==== Phase 1 ====
- msiyyh0i: [pytorchsketchrnn-look\_i16-001 | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/msiyyh0i?workspace=user-andrewlook)
    - look_i16
- xtnlxroi: [pytorchsketchrnn-look\_i16-002\_minn10 | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/xtnlxroi?workspace=user-andrewlook)
    - look_i16__minn10_epsilon1
-->
<!-- 
==== Phase 2 ====
Nov 30, Dec 1 (pre runid in filename)
- [ ] samples 0291, 0145, 0066, 0016

Dec 4:
- [ ] 5eibyllb: [atomic-puddle-15 | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/5eibyllb?workspace=user-andrewlook)
    - Dataset: look_i16__minn10_epsilon1
    - Layernorm, recurrent dropout
- [ ] t6bcos6b: [ruby-resonance-16--fix-layernorm-learnableFalse | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/t6bcos6b?workspace=user-andrewlook)

Dec 5:
- [ ] gr5r7sft: [mild-galaxy-19 | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/gr5r7sft?workspace=user-andrewlook)
- [ ] zjan5lxh: [vague-field-20--rnnlib-with-layernorm | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/zjan5lxh?workspace=user-andrewlook)

Dec 6:
- [ ] cal3jv56: [sparkling-planet-24--augmentation-and-LR-decay | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/cal3jv56?workspace=user-andrewlook)
- [ ] kb2eil37: [prime-star-23--rnnlib-recurrent-dropout | sketchrnn-pytorch – Weights & Biases](https://wandb.ai/andrewlook/sketchrnn-pytorch/runs/kb2eil37?workspace=user-andrewlook)

-->


</section>
</section>

 ]]></description>
  <category>singleline</category>
  <category>machinelearning</category>
  <category>slml</category>
  <guid>https://andrewlook.com/notebooks/blog/posts/slml_part4.html</guid>
  <pubDate>Sun, 03 Dec 2023 00:00:00 GMT</pubDate>
  <media:content url="https://i.ibb.co/BPdSrTQ/phase1-wandb-with-without-minn10.png" medium="image" type="image/png"/>
</item>
<item>
  <title>SLML Part 3 - JPEG to SVG to Stroke-3</title>
  <dc:creator>Andrew Look</dc:creator>
  <link>https://andrewlook.com/notebooks/blog/posts/slml_part3.html</link>
  <description><![CDATA[ 




<section id="slml-part-3---jpeg-to-svg-to-stroke-3" class="level1 page-columns page-full">
<h1>SLML Part 3 - JPEG to SVG to Stroke-3</h1>
<blockquote class="blockquote">
<p><em>This post is part 3 of <a href="../../projects/slml.html">“SLML” - Single-Line Machine Learning</a>.</em></p>
<p><em>To read the previous post, check out <a href="../../blog/posts/slml_part2.html">part 2</a>.</em></p>
<p><em>If you want to keep reading, here is <a href="../../blog/posts/slml_part4.html">part 4</a>.</em></p>
</blockquote>
<p>Most computer vision algorithms represent images as a rectangular grid of pixels on a screen. The model that the Magenta team trained, SketchRNN, instead interprets the drawings as a sequence of movements of a pen. They call this “stroke-3 format”, since each step in the sequence is represented by 3 values:</p>
<ul>
<li><code>delta_x</code>: how much did it move left-to-right?</li>
<li><code>delta_y</code>: how much did it move up-and-down?</li>
<li><code>lift_pen</code>: was the pen down (continuing the current stroke) or was the pen lifted (moving to the start of a new stroke)</li>
</ul>
<div class="column-page">
<p><img src="https://i.ibb.co/NLBL4v0/stroke3-turtle.png" class="img-fluid"></p>
</div>
<p>First, I had to convert my JPEG scans into the “stroke-3” format. This would involve:</p>
<ol type="1">
<li>converting the files from JPEG to SVG</li>
<li>converting SVG to stroke-3</li>
<li>simplifying the drawings to reduce the number of points</li>
</ol>
<section id="jpeg-to-svg" class="level2">
<h2 class="anchored" data-anchor-id="jpeg-to-svg">JPEG to SVG</h2>
<p>When I first started converting to SVG, I had trouble finding a tool that would give me a single, clean stroke for each line. Eventually I found a tool called <code>autotrace</code> that was able to correctly do a “centerline trace”.</p>
<div id="fig-vectorization" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vectorization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-vectorization" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-potrace" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-potrace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/sg3WjzZ/potrace-points.png" class="img-fluid figure-img" data-ref-parent="fig-vectorization" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-potrace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) <code>potrace</code>
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-vectorization" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-autotrace" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-autotrace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://i.ibb.co/D7JyvM2/autotrace-points.png" class="img-fluid figure-img" data-ref-parent="fig-vectorization" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-autotrace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) <code>autotrace</code>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vectorization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Comparison of Vectorization Tools.
</figcaption>
</figure>
</div>
</section>
<section id="svg-to-points" class="level2">
<h2 class="anchored" data-anchor-id="svg-to-points">SVG to Points</h2>
<p>Then I used a python library called <code>svgpathtools</code> to take the resulting SVG files, and convert each of the paths to a sequence of points. This step is necessary because SVG paths are often represented as Bezier curves.</p>
<p>One problem I noticed was that the drawings were represented as many separate strokes rather than one continuous line. For example, in the image below, each color represents a separate pen stroke.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.ibb.co/s21gLTQ/stroke3-01-sepstrokes.png" class="img-fluid figure-img"></p>
<figcaption>separate strokes</figcaption>
</figure>
</div>
</section>
<section id="line-simplification" class="level2">
<h2 class="anchored" data-anchor-id="line-simplification">Line Simplification</h2>
<p>Finally, I’d apply the <a href="https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm">Ramer-Douglas-Pecker</a> (“RDP”) algorithm on the resulting points, which uses an adjustable “epsilon” parameter to simplify down the drawings by reducing the number of points in a line’s path.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/69/RDP%2C_varying_epsilon.gif/440px-RDP%2C_varying_epsilon.gif" class="img-fluid figure-img"></p>
<figcaption>RDP example</figcaption>
</figure>
</div>
<p>This is important because the SketchRNN model has difficulty with sequences longer than a few hundred points, so it’s helpful to simplify the drawings down by removing some of the very fine details while preserving the overall shape.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.ibb.co/0JBLg6Q/phase1-sample-0177-epoch-01700-orig.png" class="img-fluid figure-img"></p>
<figcaption>phase1-sample-0177-epoch-01700-orig</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p><em>Next in my <a href="../../projects/slml.html">SLML</a> series is <a href="../../blog/posts/slml_part4.html">part 4</a>, where I experiment with hyperparams and datasets in training SketchRNN.</em></p>
</blockquote>


</section>
</section>

 ]]></description>
  <category>singleline</category>
  <category>machinelearning</category>
  <category>slml</category>
  <guid>https://andrewlook.com/notebooks/blog/posts/slml_part3.html</guid>
  <pubDate>Tue, 07 Nov 2023 00:00:00 GMT</pubDate>
  <media:content url="https://i.ibb.co/D7JyvM2/autotrace-points.png" medium="image" type="image/png"/>
</item>
<item>
  <title>SLML Part 2 - Assembling the Dataset</title>
  <dc:creator>Andrew Look</dc:creator>
  <link>https://andrewlook.com/notebooks/blog/posts/slml_part2.html</link>
  <description><![CDATA[ 




<section id="slml-part-2---assembling-the-dataset" class="level1 page-columns page-full">
<h1>SLML Part 2 - Assembling the Dataset</h1>
<blockquote class="blockquote">
<p><em>This post is part 2 of <a href="../../projects/slml.html">“SLML” - Single-Line Machine Learning</a>.</em></p>
<p><em>Check out the previous post, <a href="../../blog/posts/slml_part1.html">part 1</a>, for some background on why I got started training SketchRNN on my own drawings.</em></p>
<p><em>Next in the series is <a href="../../blog/posts/slml_part3.html">part 3</a>, where I convert my JPEGs into stroke-3 format in preparation for model training.</em></p>
</blockquote>
<p>For years, I’ve maintained a <a href="../../blog/posts/slml_part0.html">daily practice</a> of making single-line drawings in my sketchbooks.</p>
<div class="column-page">
<table>
<colgroup>
<col style="width: 23%">
<col style="width: 25%">
<col style="width: 23%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th><img src="https://i.ibb.co/TrdbwR2/sb48p072.jpg" class="img-fluid" alt="sb48p072"></th>
<th><img src="https://i.ibb.co/xfqZZF0/sb67p077.jpg" class="img-fluid" alt="sb67p077"><br></th>
<th><img src="https://i.ibb.co/5MtbpVL/sb67p004.jpg" class="img-fluid" alt="sb67p004"></th>
<th><img src="https://i.ibb.co/RvJ6DpK/sb38p073-gown2.jpg" class="img-fluid" alt="sb38p073-gown2"></th>
</tr>
</thead>
<tbody>
</tbody>
</table>
</div>
<p>When I finish a sketchbook, I scan its pages, number them, and put them into a similar folder structure. I marked which pages were diagrams/mindmaps/etc and put them in a subfolder called <code>notes</code> - the drawings (and watercolors) are in a folder called <code>art</code>. Unfortunately, I didn’t tag watercolors in advance.</p>
<p>Some detail on the subfolders:</p>
<ul>
<li><code>art</code> subfolders contain drawings / watercolors.</li>
<li><code>notes</code> subfolders contain diagrams, mindmaps, and anything else technical I’m working on. I used to have a separate notebook for these, but I got tired of carrying around an extra sketchbook everywhere I go, so at some point they just ended up merging with my art practice.</li>
<li><code>xtra</code> contains any bad scans or funny bloopers from my scanning process. I kind of like the scans with my hands in them and thought they might be useful for a future project, so I kept them in their own folder.</li>
<li><code>cover</code> contains a scan of the notebook cover. Each cover at least has a post-it with the sketchbook ID number and the start/end dates of when I used that sketchbook. Often I’ll make a little collage on the front of the notebook with airplane tickets / stickers / scraps of paper relevant to somewhere I traveled when I used that sketchbook.</li>
</ul>
<div class="column-body">
<p><img src="https://i.ibb.co/BjQhB4t/folder-list-view.png" class="img-fluid"></p>
</div>
<!-- 
![folder-icons](https://i.ibb.co/RD2K7SJ/folder-icons.png)
![short-sb-flipping-art2](https://i.ibb.co/6s5qxJp/short-sb-flipping-art2.gif)
-->
<section id="the-problem-filtering" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-problem-filtering">The Problem: Filtering</h2>
<p>My first problem is that the “art” sections of my sketchbooks don’t just contain single-line drawings. They also contain watercolor paintings, since I don’t exclusively fill my sketchbooks with single-line drawings.</p>
<div class="column-page">
<table>
<colgroup>
<col style="width: 25%">
<col style="width: 29%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th><img src="https://i.ibb.co/Qrf23tn/sb69p086-wc.jpg" class="img-fluid" alt="sb69p086-wc"></th>
<th><img src="https://i.ibb.co/Hd4zsKb/sb67p051-sparky.jpg" class="img-fluid" alt="sb67p051-sparky"></th>
<th><img src="https://i.ibb.co/0Zr2MNT/sb69p006.jpg" class="img-fluid" alt="sb69p006"></th>
<th><img src="https://i.ibb.co/sFkp4D8/sb35p013.jpg" class="img-fluid" alt="sb35p013"></th>
</tr>
</thead>
<tbody>
</tbody>
</table>
</div>
</section>
<section id="measuring-visual-similarity-with-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="measuring-visual-similarity-with-embeddings">Measuring Visual Similarity with Embeddings</h2>
<p>Looking for a scalable way to filter the drawings out of my art scans, I decided to try computing visual embeddings.</p>
<!-- I only labeled "notes" and "art" separately but didn't separate "watercolors" into their own category distinct from "drawings". The labeled data I had turned out to be useful. -->
<p>I decided to fine-tune a vision model on my dataset to distinguish “art” pages from “notes” pages. My hope was that the model would learn to distinguish some features within the visual domain of my scanned sketchbook pages. Even though I don’t intend to use the predictions of an algorithm trained to classify art-vs-notes, this will produce more meaningful embeddings.</p>
<!-- In the past, I've found that vision models trained on standard datasets like ImageNet can produce nonsensical results when they're shown black-and-white scanned images. ImageNet consists mostly of photographs, which contain a wide distribution of colors - there aren't a lot of images in there where the majority of pixels are white. -->
<p>Ultimately, what I want to see is that when I take the embeddings of my sketchbook pages and cluster them, the notes and drawings should neatly separate into separate clusters. Also, I want the watercolors to cleanly separate into their own clusters, away from the single-line drawings.</p>
</section>
<section id="inspecting-the-embeddings" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="inspecting-the-embeddings">Inspecting the Embeddings</h2>
<p>After I fine-tuned my vision model, I computed the embeddings for all the sketchbook pages I had on hand. As a sanity check, I want to verify that the embedding space has cleanly separated regions for the labels is was trained on.</p>
<p>It’s hard to visualize 512 dimensions, so the T-SNE dimensionality reduction is a good visualization technique to view the data. T-SNE projects the higher number of dimensions into 2D or 3D, with the goal that any 2 points close together in high-dimensional space are close together in lower-dimensional space.</p>
<p>I colorized the points in the resulting graph based on their label, so it’s easy to see at a glance that embedding space has learned something useful about this dataset, as I hoped.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://i.ibb.co/NmMdwJS/tsne-folders.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">T-SNE representation of sketchbook pages. <code>art</code> pages are blue, <code>notes</code> pages are orange, and <code>cover</code> pages are brown.</figcaption>
</figure>
</div>
</section>
<section id="clustering-to-identify-sub-groups" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="clustering-to-identify-sub-groups">Clustering to Identify Sub-Groups</h2>
<p>Now I’m hoping to find the watercolors in the dataset. Given that the embedding space captures visual similarity, I want to identify groups of pages that are similar to each other. This lets me label data in bulk, rather than going through the images one-by-one. K-Means Clustering was a quick solution for this. I can decide to ask for <code>K</code> distinct “clusters”, and the algorithm picks <code>K</code> “centroids”.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://shabal.in/visuals/kmeans/bottom.gif" class="img-fluid figure-img" width="600"></p>
<figcaption class="margin-caption">Given a request for <code>K</code> clusters, pick <code>K</code> random points as the starting centroids. Then, repeat these steps for <code>N</code> iterations: For each point, select the nearest centroid point and compute the distance between these two. Adjusts the centroids with the goal of minimizing the total distance from all points to their nearest centroids.</figcaption>
</figure>
</div>
<!-- https://shabal.in/visuals/kmeans/5.html -->
</section>
<section id="using-clusters-to-classify" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="using-clusters-to-classify">Using Clusters to Classify</h2>
<p>After some experimentation, I found that using K=16 produced groupings that would be useful to me. Each row in the image below consists of the top images from one cluster.</p>
<div class="column-page">
<p><img src="https://i.ibb.co/Vj5kRw3/download.png" class="img-fluid"></p>
</div>
<p>Since each cluster’s centroid is a vector in the same space as the image embeddings, I can “classify” any image embedding by doing a nearest-neighbor search of comparing the embedding of the query image to each of the cluster centroid vectors, and choosing the cluster centroid with the smallest distance. This is also called a K-Nearest Neighbor classifier.</p>
<p>When I applied this process to each of the images, I glanced at a t-SNE colorized by these clusters:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://i.ibb.co/hsbWBGs/tsne-clusters20.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">T-SNE representation of sketchbook pages, colored by which of the 16 clusters they were classified by K-Means using the pages’ visual embeddings.</figcaption>
</figure>
</div>
<!-- I've also saved the centroids, so that if I add new images I can map them to the same set of clusters. -->
</section>
<section id="making-sense-of-the-clusters" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="making-sense-of-the-clusters">Making Sense of the Clusters</h2>
<p>I went through and grouped these clusters based on what I wanted to identify automatically. In particular, I wanted to take only the drawings and separate them from the watercolors. After inspecting the clusters, I also realized that there were a number of bad scans / incorrectly cropped images that I wanted to exclude as well.</p>
<div class="column-page">
<p><img src="https://i.ibb.co/R3fg2X3/download-1.png" class="img-fluid"></p>
<p><img src="https://i.ibb.co/hYq2X4p/download-2.png" class="img-fluid"></p>
<p><img src="https://i.ibb.co/87L8R80/download-3.png" class="img-fluid"></p>
<p><img src="https://i.ibb.co/Pm4t6j7/download-5.png" class="img-fluid"></p>
<p><img src="https://i.ibb.co/k8LNmGN/download-4.png" class="img-fluid"></p>
</div>
<p>I went through the dataset again, and based on the cluster assigned to each image, I assigned it a higher-level group based on the hand-mapping of clusters above.</p>
<p>When I looked at the t-SNE colorized by this higher-level cluster mapping, it was encouraging to see that my embedding space had neatly separated the groups I cared about:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://i.ibb.co/wQPXG0Q/tsne-clusters5.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">T-SNE representation of pages, colored by cluster group. <code>drawings</code> are purple, <code>notes</code> are dark green, <code>watercolors</code> are yellow, <code>bad scans</code> are blue, <code>covers</code> are light green.</figcaption>
</figure>
</div>
<p>Since my images were tagged with these higher-level groups, I was able to select only the drawings, and begin converting them into usable training data.</p>
<blockquote class="blockquote">
<p><em>Next in my <a href="../../projects/slml.html">SLML</a> series is <a href="../../blog/posts/slml_part3.html">part 3</a>, where I convert my JPEGs into stroke-3 format in preparation for model training.</em></p>
</blockquote>


</section>
</section>

 ]]></description>
  <category>singleline</category>
  <category>machinelearning</category>
  <category>slml</category>
  <guid>https://andrewlook.com/notebooks/blog/posts/slml_part2.html</guid>
  <pubDate>Wed, 01 Nov 2023 00:00:00 GMT</pubDate>
  <media:content url="https://i.ibb.co/Vj5kRw3/download.png" medium="image" type="image/png"/>
</item>
<item>
  <title>SLML Part 1 - Why I Decided to Train SketchRNN on My Drawings</title>
  <dc:creator>Andrew Look</dc:creator>
  <link>https://andrewlook.com/notebooks/blog/posts/slml_part1.html</link>
  <description><![CDATA[ 




<section id="slml-part-1---why-i-decided-to-train-sketchrnn-on-my-drawings" class="level1 page-columns page-full">
<h1>SLML Part 1 - Why I Decided to Train SketchRNN on My Drawings</h1>
<blockquote class="blockquote">
<p><em>This post is part 1 of <a href="../../projects/slml.html">“SLML” - Single-Line Machine Learning</a>.</em></p>
<p><em>If you want some context on how I got into single-line drawings, check out the <a href="../../blog/posts/slml_part0.html">part 0</a>.</em></p>
<p><em>Next in the series is <a href="../../blog/posts/slml_part2.html">part 2</a> where I assemble the training data, and filter it using visual embeddings.</em></p>
</blockquote>
<section id="discovering-sketchrnn" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="discovering-sketchrnn">Discovering SketchRNN</h2>
<p>David Ha and Doug Eck from the Magenta team at Google had crowdsourced over 100,000 drawings using a game call “Quick, Draw”. The game would give users a prompt such as “Yoga” or “Pig” and users would have 20 seconds to draw it in the browser. This produced a dataset of line drawings for each category. Then, they trained a model called SketchRNN on these drawings. They trained a separate model for each category that users were prompted to draw, so there’s a separate model trained for “yoga”, “pig”, etc.</p>
<p>I was fascinated by Magenta’s <a href="https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html">SketchRNN demo</a> where you start a drawing, and the SketchRNN model tries continuously to complete it in new ways. It makes me think about the conditional probability that evolves as a drawing progresses. Given that one line exists on the page, how does that influence what line the artist is most likely to draw next?</p>
<p>My favorite example is to select the yoga model, and then draw a rhombus shape simulating a yoga mat. I love watching stick figures emerge in various poses, all centered on and interacting with the yoga mat.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://i.ibb.co/fSqZM1j/magenta-demo-01-draw-and-gen.gif" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">magenta-demo-01-draw-and-gen</figcaption>
</figure>
</div>
<p>Some of the same authors from the Magenta paper collaborated with distill.pub, using the same SketchRNN model trained on a handwriting dataset, to publish an explorable explanation called <a href="https://distill.pub/2016/handwriting/">Experiments in Handwriting with a Neural Network</a>.</p>
</section>
<section id="why-train-a-model-on-my-single-line-drawings" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="why-train-a-model-on-my-single-line-drawings">Why train a model on my single-line drawings?</h2>
<p>I wanted to apply SketchRNN to my own single-line drawings.</p>
<p>I’d considered training an ML model on my single-line drawings, but the pixel-based GAN’s and VAE’s available in 2017/2018 when I started thinking about this didn’t seem like they’d yield good results on images that are mostly composed of white pixels with narrow black lines interspersed. What SketchRNN produces sequences, the generated results could be animated. I see single-line drawing as a sort of performance. A video of me drawing a single-line is inherently a proof that I didn’t lift my pen from the page.</p>
<p>It struck me that it could give a new window into my own drawing style. If my own drawing style evolves slowly over time, would I be able to notice the difference between a model trained on drawings I made recently from one trained on drawings I made several years ago?</p>
<p>How cool would it be to have an interface, similar to Andy Muatuschak’s <a href="https://andymatuschak.org/scrying-pen/">scrying pen</a>, where I could start drawing and see live completions, showing me how the probability space of subsequent strokes in my drawing is changing?</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://i.ibb.co/4YrMbnG/Screen-Cast-2024-03-08-at-8-53-56-PM.gif" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">scrying pen demo</figcaption>
</figure>
</div>
<!--![distill-draw](https://i.ibb.co/q7gn1md/distill-draw.gif) -->
<p>What new ideas might I get from turning the “variability” up, like the slider in distill.pub’s <a href="https://distill.pub/2016/handwriting/">handwriting demo</a>, and generating new drawings based on my old ones? <img src="https://i.ibb.co/h7nQM0n/distill-variation.gif" class="img-fluid" alt="distill-variation"> Or running stroke prediction along the length of a drawing:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://i.ibb.co/wpfgwgf/distill-vary-strokes.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">distill-vary-strokes</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://i.ibb.co/YWHGtBF/distill-vary-strokes-legend.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">distill-vary-strokes-legend</figcaption>
</figure>
</div>
<!-- ![distill-vary-strokes](https://i.ibb.co/yVjZM12/distill-vary-strokes.gif) -->
</section>
<section id="getting-started" class="level2">
<h2 class="anchored" data-anchor-id="getting-started">Getting Started</h2>
<p>When I reached out to the authors of SketchRNN, they estimated that I’d need several thousand examples in order to train a model. I only had a few hundred at the time. But I kept making new drawings in my sketchbooks, and numbering the pages and the sketchbooks so that I could scan them and store them in a consistent way. In the back of my mind, I held on to the idea that one day I’d have enough drawings to train a model on them.</p>
<p>Several years went by. More sketchbooks accumulated.</p>
<p>Eventually, I ran a file count and saw a few thousand distinct drawings. I was finally ready to get started. I was going to need:</p>
<ol type="1">
<li>at least a thousand drawings</li>
<li>the ability to convert my scanned drawings into stroke-3 format</li>
<li>to train a model on my drawings, and experiment until I found a configuration producing results that I liked.</li>
</ol>
<blockquote class="blockquote">
<p><em>Next in my <a href="../../projects/slml.html">SLML</a> series is <a href="../../blog/posts/slml_part2.html">part 2</a>, where I convert my JPEG scans into vector representations in preparation for training SketchRNN.</em></p>
</blockquote>


</section>
</section>

 ]]></description>
  <category>singleline</category>
  <category>machinelearning</category>
  <category>slml</category>
  <guid>https://andrewlook.com/notebooks/blog/posts/slml_part1.html</guid>
  <pubDate>Fri, 27 Oct 2023 00:00:00 GMT</pubDate>
  <media:content url="https://i.ibb.co/h7nQM0n/distill-variation.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>SLML Part 0 - My Single-Line Drawing Practice</title>
  <dc:creator>Andrew Look</dc:creator>
  <link>https://andrewlook.com/notebooks/blog/posts/slml_part0.html</link>
  <description><![CDATA[ 




<!-- TODO: image -->
<section id="my-single-line-drawing-practice" class="level1">
<h1>My Single-Line Drawing Practice</h1>
<blockquote class="blockquote">
<p><em>This post is the background to my series <a href="../../projects/slml.html">“SLML” - Single-Line Machine Learning</a>.</em></p>
<p><em>Next in the series is <a href="../../blog/posts/slml_part1.html">part 1</a>, where I discovered SketchRNN and got curious about applying it to my own artwork.</em></p>
</blockquote>
<p>I started experimenting with single-line drawings around 2017. I’d been painting and drawing and experimenting with Machine Learning art for around 5 years at that point, but my art hobby had started to feel like a chore. I was taking on more and more ambitious painting projects, and there wasn’t a lot of simplicity or repetition in my art practice. A lot of my art felt premeditated, as I’d print out what I wanted to paint and then “execute” it on the canvas, painstakingly trying to represent what I’d decided to paint as accurately as possible.</p>
<p>I got tired of painting but wanted to keep up my art practice, so I started just carrying around a sketchbook. Since I’d noticed a tendency to overthink my drawings I thought a pencil would give me too much freedom to erase, so I only brought a pen with me. I found myself wanting to practice the challenging parts of paintings that I’d struggled with, particularly drawing people and faces.</p>
<p>Friends advised me that adding a constraint could help me to iterate quickly and not overthink my drawings. Since one of my patterns had been to spend too much time on individual drawings, I decided to adopt an extreme constraint: as soon as I lifted the pen from the page, I’d call the drawing done.</p>
<p>I was drawn at first to the technical challenge. As a painter I’d already been learning about facial anatomy in an attempt to improve my drawings. Now I needed not only to know enough about anatomy to visualize a person or face I wanted to draw in three dimensions, but also to pick a path across that surface to make a finished drawing.</p>
<p>As a painter I’d grown accustomed to shading and blending to add or remove contrast. Once I started working only with lines, I had to find new tricks. Negative space in a drawing can create a good shadow. Concentric circles creates an illusion of depth. Putting lines close together makes something look darker.</p>
<p>Over time, other aspects of single-line drawing pulled me in further. I found how much beauty there was to be found in what to leave out of a drawing. I learned how satisfying it is to choose just the essential lines needed to imply a facial expression or a pose.</p>
<blockquote class="blockquote">
<p><em>If you want to keep reading, check out <a href="../../blog/posts/slml_part1.html">part 1</a> of my <a href="../../projects/slml.html">SLML</a> series, where I discovered SketchRNN and got curious about applying it to my own artwork.</em></p>
</blockquote>
<!--
Technical Challenge
- The challenge of drawing from imagination and memory.
- Knowing enough anatomy to be able to improvise and arrive at something realistic.

Exercise in Minimalism
- The creativity involved in knowing what to leave out.
- Finding the essential lines to represent someone or something.
- In a way it’s a kind of compression - what’s the fewest number of lines I can make to imply my image in a viewer’s mind?

[[BlindContourDrawing]]
-->


</section>

 ]]></description>
  <category>singleline</category>
  <category>machinelearning</category>
  <category>slml</category>
  <guid>https://andrewlook.com/notebooks/blog/posts/slml_part0.html</guid>
  <pubDate>Fri, 20 Oct 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>A Tour of AI Art in 2018</title>
  <dc:creator>Andrew Look</dc:creator>
  <link>https://andrewlook.com/notebooks/blog/posts/2018_ai_art_tour.html</link>
  <description><![CDATA[ 




<p>By the end of this tour, my goal is to help you understand how some existing artists are incorporating AI tools and concepts into their work, and how using AI art tools might <em>augment</em> your creativity.</p>
<p>Disclaimer: I’m not a real expert on computer vision, neuroscience or art. But I <em>have</em> spent a couple years learning about the space, so I’ll do my best to explain things as I understand them so far.</p>
<p>For those more familiar with AI, I plan on making some broad generalizations in the interest of brevity. If you’re interested in the technical details, I’ve done my best to include relevant links on where to go deeper.</p>
<section id="what-is-ai-art" class="level2">
<h2 class="anchored" data-anchor-id="what-is-ai-art">What is AI art?</h2>
<p>Many artistic tools come from people striving to understand how algorithms affect our lives. I’ve distilled a framework to understand different types of AI Art, and am sharing it in the hopes that it makes the concepts easier to discuss and compare.</p>
<p>Here are 3 ways I can think of to define “AI Art”:</p>
<ol type="1">
<li><strong>Making creative use of outputs from an AI <em>Tool</em></strong></li>
<li><strong>Leveraging how AI represents information to deliberately craft an <em>Effect</em></strong></li>
<li><strong>Exploring the <em>Concepts</em> within AI and what they mean for us</strong></li>
</ol>
</section>
<section id="some-definitions" class="level2">
<h2 class="anchored" data-anchor-id="some-definitions">Some Definitions</h2>
<p>Given the amount of cultural baggage we all associate with words like “AI” and “art”, it’s worth clarifying some definitions up front. “AI Art” has stuck as a phrase to describe an emerging category of creative work, but it might be more accurate to call it “Machine Learning Art.”</p>
<p>The art I’m discussing here is mostly made by humans using statistical methods, not by sentient robots with their own creative ideas. Sorry to disappoint, but this post isn’t dedicated to anthropomorphic machines that paint:</p>
<blockquote class="blockquote">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/8P7fmIpUYBwHgjo322/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>not this</figcaption>
</figure>
</div>
<p>SPIRAL by Deepmind (<a href="https://deepmind.com/blog/learning-to-generate-images/">blog</a>, <a href="https://deepmind.com/documents/183/SPIRAL.pdf">pdf</a>, <a href="https://www.youtube.com/watch?v=iSyvwAwa7vk">youtube</a>)</p>
</blockquote>
<section id="ai" class="level3">
<h3 class="anchored" data-anchor-id="ai">AI</h3>
<p>AI is a term used in so many contexts, that it’s a bit of a loaded term. Most mentions of AI can be bucketed into one of two broad categories:</p>
<ul>
<li><strong>General AI</strong>: The futuristic aspiration of computers that experience consciousness</li>
<li><strong>Specialized AI</strong>: Systems using statistics to learn patterns from data in order to make predictions.</li>
</ul>
<p>I’ll be focused on artistic applications of techniques that have sprung out of real-world research from the category of <strong>Specialized AI</strong>, also known as <strong>Machine Learning</strong>.</p>
</section>
</section>
<section id="machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning">Machine Learning</h2>
<p>Machine Learning (usually) refers to a type of algorithm that (1) is built to perform a clearly-defined <strong>task</strong>, and (2) <strong>learns</strong> patterns and relationships from <strong>training data</strong></p>
<!-- related: Explainability / Interpretability, Algorithmic Accountability -->
<p>For example, imagine that we want to <a href="https://medium.com/@kabab/linear-regression-with-python-d4e10887ca43">predict housing prices</a>. We’d probably start with a spreadsheet of prices we know to be true, alongside features for each house that we expect to be related to the price. If we plot price vs.&nbsp;size on this toy dataset, we can start to see a slope.</p>
<div>
<p><img width="22%" style="display: inline" alt="housing prices table" src="https://cdn-images-1.medium.com/max/1280/1*TE_oNKRRek5io8v_-uZ9PQ.png"> <img width="75%" style="display: inline" alt="housing prices scatter plot" src="https://cdn-images-1.medium.com/max/1280/1*HUJzcLczeBFRdPPZ5HMcbw.png"></p>
</div>
<p>If we were to draw a line through these points, the “slope” would be what you multiply the <code>size</code> by in order to get the <code>price</code> (plus a “bias” term, if the slope doesn’t meet the Y-axis right at zero). The <strong>objective</strong> of an ML algorithm is to find a line to draw through these points that <strong>minimizes the error</strong> in its predictions. You can visualize the <strong>error</strong> as the distance from each real data point’s <code>Y</code> coordinate to the <strong>prediction</strong>: the line’s value at the corresponding <code>X</code> coordinate.</p>
<div>
<p><img width="45%" style="display: inline" alt="error bars" src="https://cdn-images-1.medium.com/max/1280/1*iBDH0gBBJNs-oLqUT17yng.png"> <img width="45%" style="display: inline" alt="iterative learning" src="https://cdn-images-1.medium.com/max/1280/1*xc5CSmK9d8oeKYxKxenEGg.gif"></p>
</div>
<p>“Art is everything that you don’t have to do.” - <a href="https://twitter.com/BBC6Music/status/648200818697572352?s=20">brian eno</a></p>
<p>Whole books could be written in an attempt to define art, so I won’t attempt an exhaustive definition.</p>
<blockquote class="blockquote">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/f/f6/Duchamp_Fountaine.jpg" width="400"></p>
<p>“Fountain,” Marcel Duchamp, 1917 (<a href="https://en.wikipedia.org/wiki/Marcel_Duchamp">wikipedia</a>)</p>
</blockquote>
<p>The truth is that art can be defined by whoever is creating it, and art history is full of boundary-pushing acts leading onlookers to ask “but is it art?”</p>
</section>
<section id="back-to-ai-art" class="level2">
<h2 class="anchored" data-anchor-id="back-to-ai-art">Back to AI Art</h2>
<p>Now I’ll step through three possible ways to define AI art and provide some examples along the way.</p>
<ol type="1">
<li><strong>Making creative use of outputs from an AI <em>Tool</em></strong></li>
<li><strong>Leveraging how AI represents information to deliberately craft an <em>Effect</em></strong></li>
<li><strong>Exploring the <em>Concepts</em> within AI and what they mean for us</strong></li>
</ol>
<!--
These definitions actually mirror the progression of my own art practice. I'll step through them in an attempt to share how my understanding of the space has evolved since I first started.
-->
<hr>
</section>
<section id="definition-1-making-creative-use-of-outputs-from-an-ai-tool" class="level2">
<h2 class="anchored" data-anchor-id="definition-1-making-creative-use-of-outputs-from-an-ai-tool">Definition #1: Making creative use of outputs from an AI <em>Tool</em></h2>
<p>The simplest definition of AI art is any artifact generated from a tool the makes use of AI, whether or not the artist makes this a central part of the work.</p>
<p>Apps such as Prisma, for example, make it easy for anyone to take a photo on their phone and render it in a painterly style.</p>
<p>Even without much context on how they work, easy-to-use tools such as <a href="https://affinelayer.com/pixsrv/">pix2pix</a> can be fun to play with and yield weird results.</p>
<blockquote class="blockquote">
<p><img src="https://media.giphy.com/media/fik7beSODmO75YI6Qd/giphy.gif" class="img-fluid" alt="bread_cat">?&gt;</p>
<p><em>“Image-to-Image Translation with Conditional Adversarial Networks”</em>, Isola et al, 2017 (<a href="https://arxiv.org/abs/1611.07004">pdf</a>)</p>
</blockquote>
<p>We’ll look at some more in-depth approaches later (time permitting), but for now this is just fun.</p>
<blockquote class="blockquote">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/wHYLEo4Z7kPJwlbO9U/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>pix2pix_pyramid_cat</figcaption>
</figure>
</div>
</blockquote>
<hr>
</section>
<section id="definition-2-leveraging-how-ai-represents-information-to-deliberately-craft-an-effect" class="level2">
<h2 class="anchored" data-anchor-id="definition-2-leveraging-how-ai-represents-information-to-deliberately-craft-an-effect">Definition #2: <strong>Leveraging how AI represents information to deliberately craft an effect</strong></h2>
<p>A slightly more complex definition of AI art is one in which an artist uses his or her understanding of the machine learning internals to achieve a specific effect. For example, it’s possible to make some interesting visual artifacts by inspecting how machine learning algorithms represent information.</p>
<blockquote class="blockquote">
<p><img src="https://courses.cs.washington.edu/courses/cse576/13sp/images/eigenfaces.png" width="400"></p>
<p><a href="https://courses.cs.washington.edu/courses/cse576/13sp/projects/project3/">source</a></p>
</blockquote>
<p>In fact, the spooky faces came from a visualization of Eigenfaces, which sought to “learn” how to represent faces in a “latent space”. For me, this early facila recognition output brings to mind one of my favorite quotes by Brian Eno (who I’m quoting more than once in this piece since he’s had so much to say about the overlap of technology and art).</p>
<blockquote class="blockquote">
<p><em>“<strong>Whatever you now find weird, ugly, uncomfortable, and nasty about a new medium will surely become its signature.</strong> CD distortion, the jitteriness of digital video, the crap sound of 8-bit, all of these will be cherished and emulated as soon as they can be avoided. It’s the sound of failure. So much modern art is the sound of things going out of control. Out of a medium, pushing to its limits and breaking apart.”</em> - <a href="https://www.goodreads.com/quotes/649039-whatever-you-now-find-weird-ugly-uncomfortable-and-nasty-about">Brian Eno</a></p>
</blockquote>
<hr>
<p>Recent generative algorithms have rapidly improved their ability both to learn <strong>latent spaces</strong>, and to <strong>generate</strong> images from any point in these latent spaces.</p>
<blockquote class="blockquote">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://onionesquereality.files.wordpress.com/2009/02/face_space.jpg" class="img-fluid figure-img"></p>
<figcaption>facespace</figcaption>
</figure>
</div>
<p><em>Face Recognition using Eigenfaces,</em> Turk et al, 1991 <a href="http://www.cs.ucsb.edu/~mturk/Papers/mturk-CVPR91.pdf">pdf</a></p>
</blockquote>
<p>Methods of <strong>encoding</strong> real images into vectors representing a point in latent space have since improved, as have the methods of <strong>decoding</strong> latent vectors back into realistic images.</p>
<p>When we <strong>interpolate</strong> between two points in the latent space, we can smoothly generate images at each point along the way. In the uncanny example below, I find it striking that (almost) every point in between looks like a person. Looking at this work, I can’t help thinking that these algorithms are showing us something about our innate similarity to all other humans.</p>
<blockquote class="blockquote">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/1Bgck5vkuyNxSFPNmJ/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>celeb_1hr_man</figcaption>
</figure>
</div>
<p><em>Progressive Growing of GANs for Improved Quality, Stability, and Variation</em>, Karras et al, 2018 (<a href="http://research.nvidia.com/publication/2017-10_Progressive-Growing-of">pdf</a>, <a href="https://www.youtube.com/watch?v=36lE9tV9vm0">youtube</a>)</p>
</blockquote>
<p>One striking example of a completely new type of tool leveraging this understanding is called <a href="https://vusd.github.io/toposketch/">Toposketch</a>. It allows visual navigation of latent spaces in order to gain fine-grained control over the generated artifacts. This makes me wonder what kinds of creative tools could be built to leverage the information that machines are able to derive from the increaing volume of data that’s available to us today.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/5BYqBxH66X3UobP3SH/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>toposketch_man_loop</figcaption>
</figure>
</div>
<hr>
</section>
<section id="definition-3-exploring-the-concepts-within-ai-and-what-they-means-for-us" class="level2">
<h2 class="anchored" data-anchor-id="definition-3-exploring-the-concepts-within-ai-and-what-they-means-for-us">Definition #3: <strong>Exploring the <em>Concepts</em> within AI and what they means for us</strong></h2>
<p>Among other things, this could encompass exploring AI’s relationships to:</p>
<ul>
<li>our society</li>
<li>the individual</li>
<li>the nature of consciousness</li>
</ul>
<section id="example-treachery-of-imagenet" class="level3">
<h3 class="anchored" data-anchor-id="example-treachery-of-imagenet">Example: Treachery of Imagenet</h3>
<p>Tom White made some beautifully designed prints, each of which fools a computer vision network into believing it’s actually a picture of a specific object (see the predicted categories to the right of the image below).</p>
<blockquote class="blockquote">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/andrewlook/tour-of-ai-art/raw/master/public/images/tom_white_adversarial_posters.jpg" class="img-fluid figure-img"></p>
<figcaption>img</figcaption>
</figure>
</div>
<p><a href="https://medium.com/artists-and-machine-intelligence/perception-engines-8a46bc598d57">source</a></p>
</blockquote>
<p>There’s a new field of security concerned with adversarial machine learning, concerned with how attackers can fool neural networks into misinterpreting what they see. It’s not intuitive, but the consequences can be serious - imagine a self-driving car fooled into missing a stop sign. I admire how Tom White took a complex concept and used it to make a visually appealing piece that sparks curiosity and raises awareness about concepts that may affect our lives but don’t have easy conversational entry points for outsiders.</p>
</section>
<section id="example-simulating-how-humans-draw" class="level3">
<h3 class="anchored" data-anchor-id="example-simulating-how-humans-draw">Example: Simulating How Humans Draw</h3>
<p>I’m fascinated when researchers attempt to mimic how humans draw. In particular, I’m interested in how the researchers set up an algorithm to mimic the process of drawing - they have to choose what happens next, how much the first part of the drawing affects what happens next, and how to define success.</p>
<blockquote class="blockquote">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/OjY1YSX6jFROm02W0J/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>simulating how humans draw</figcaption>
</figure>
</div>
<p><a href="https://deepmind.com/blog/learning-to-generate-images/">source</a></p>
</blockquote>
<p>In my analog artworks, I’ve noticed that sometimes I start a drawing not knowing how it will end up. Small accidents or random movements early in the drawing lead me to see something unexpected, adjust my course and capture what I saw. Oddly enough, thinking about the rigid framing of machine learning problems (input: view of the canvas, output: X,Y,Z coordinates of brush movement) has led to some fun experiments. For example, what happens if i close my eyes and draw?</p>
<hr>
</section>
</section>
<section id="in-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="in-conclusion">In Conclusion</h2>
<p>Given that there are several ways to define AI art (and I’m sure my list is not comprehensive), my hope is that we can broaden our view of what AI art means. While it does include fun images made using new mobile apps, I’d also like to think that there’s some more profound works out there. Works that can surprise people, help them understand the role of new technologies in their daily lives, and stoke their curiosity to learn more.</p>
<blockquote class="blockquote">
<p>“Stop thinking about art works as objects, and start thinking about them as triggers for experiences. … what makes a work of art ‘good’ for you is not something that is already ‘inside’ it, but something that happens inside you — so the value of the work lies in the degree to which it can help you have the kind of experience that you call art.”</p>
<p>Brian Eno, via <a href="https://www.brainpickings.org/2013/05/15/brian-eno-diary-art/">brainpickings</a></p>
</blockquote>


</section>

 ]]></description>
  <category>machinelearning</category>
  <category>art</category>
  <guid>https://andrewlook.com/notebooks/blog/posts/2018_ai_art_tour.html</guid>
  <pubDate>Wed, 18 Apr 2018 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
