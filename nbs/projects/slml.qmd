---
title: Single-Line Machine Learning
description: Overview of my project training ML models on a dataset of single-line drawings.
categories: [singleline, machinelearning, slml]
author: Andrew Look
date: 2023-10-25
date-modified: 2024-03-06
image: https://i.ibb.co/Vj5kRw3/download.png
comments:
  hypothesis:
    theme: clean
    openSidebar: false
cap-location: bottom
sidebar: false
toc: true
lightbox: auto
listing:
  - id: posts-list
    contents: "../blog/posts/slml_*.qmd"
    type: table
    sort: "date asc"
    date-format: iso
    sort-ui: false
    filter-ui: false
    categories: false
    fields: [title]
---

# Single-Line Machine Learning

Lately I've been working on training ML models to generate single-line drawings in my style. I've open-sourced the code for my [models](https://github.com/andrewlook/singleline_models) and the code I used to prepare the [dataset](https://github.com/andrewlook/singleline_dataset).

I've started writing deep dives for each phase of the project.

:::{#posts-list}
:::

This page includes a broader overview of the story as a whole, linking to the individual sections for more detail.

## Why This Project

In [part 0](/blog/posts/slml_part0.qmd) I share how I got started making single-line drawings, and why I found them interesting enough to make them as a daily practice.

:::{.column-body-outset}
{{< video "https://storage.googleapis.com/andrewlook-art-assets/andrewlook.com/videos/sb20p077-drawing-vid.mp4" >}}
:::

In [part 1 - Discovering SketchRNN](/blog/posts/slml_part1.qmd) I cover how SketchRNN captured my imagination and why I decided to try training it on my own data. For example, I imagined turning the “variability” up, and generating new drawings based on my old ones?

:::{.column-body-outset}
![distill.pub's [handwriting demo](https://distill.pub/2016/handwriting/)](https://i.ibb.co/h7nQM0n/distill-variation.gif)
:::

I hit my first roadbloack when I reached out to the authors of SketchRNN. They estimated that I’d need thousands of examples in order to train a model, but I only had a few hundred at the time.

I decided to keep making drawings in my sketchbooks, numbering the pages, and scanning them to store with a standardized file naming scheme.

:::{.column-body-outset}
{{< video "https://storage.googleapis.com/andrewlook-art-assets/andrewlook.com/videos/hustl-20231207023837-sb77-scanning.mp4" >}}
:::

In the back of my mind, I held on to the idea that one day I’d have enough drawings to train a model on them.

Several years went by. More sketchbooks accumulated. Eventually, I ran a file count and saw a few thousand distinct drawings.

:::{#fig-lines .column-body-outset layout-ncol=4}
![](https://i.ibb.co/TrdbwR2/sb48p072.jpg)

![](https://i.ibb.co/xfqZZF0/sb67p077.jpg)

![](https://i.ibb.co/5MtbpVL/sb67p004.jpg)

![](https://i.ibb.co/RvJ6DpK/sb38p073-gown2.jpg)

Some example single-line drawings from my sketchbooks.
:::

## Preparing the Dataset

I was finally ready to get started. I was going to need:

1. a thousand JPEGs of my drawings (at least)
2. a stroke-3 conversion process for my JPEG scans
3. a SketchRNN model and trainer
4. the patience to experiment with hyperparameters

I started by collecting my sketchbook page JPEGs into usable training data.

:::{#fig-watercolors .column-body-outset layout-ncol=4}
![sb26p068-purple-hair](https://i.ibb.co/KFPv1dp/sb26p068-purple-hair.jpg)

![sb55p069-color](https://i.ibb.co/HqsynCv/sb55p069-color.jpg)

![sb26p098-pickle-toast](https://i.ibb.co/jW8rBNX/sb26p098-pickle-toast.jpg)

![sb26p069-red-nose](https://i.ibb.co/ScRSctH/sb26p069-red-nose.jpg)

Some watercolor examples from my sketchbooks.
:::

In [part 2 - Embedding Filtering](/blog/posts/slml_part2.qmd), I cover how I'm using embeddings to filter my dataset of drawings. I used embeddings to solve the problem of filtering everything out of my sketchbook data that wasn't a single-line drawing - particularly my watercolors.

I also made an exploratory browser to visualize the embedding space of the drawings, and published it at [projector.andrewlook.com](https://projector.andrewlook.com). Here's a demo video:

:::{.column-body-outset}
{{< video "https://storage.googleapis.com/andrewlook-art-assets/andrewlook.com/videos/emb-projector.mp4" >}}
:::

In [part 3 - Dataset Vectorization](/blog/posts/slml_part3.qmd), I cover how I'm vectorizing the scans to prepare them for RNN/transformer training. At this stage in the process, my drawings were converted to centerline-traced vector paths, but they were represented as a series of separate strokes. It's visible in @fig-sepstrokes how the strokes are out of order, since strokes don't start where the previous stroke left off.

:::{#fig-sepstrokes .column-body}
{{< video https://storage.googleapis.com/andrewlook-art-assets/andrewlook.com/videos/stroke3-sepstrokes1.mp4 >}}

Example of separate strokes after my vectorization process.
:::
<!-- ![separate strokes](https://i.ibb.co/s21gLTQ/stroke3-01-sepstrokes.png) -->

## SketchRNN Experiments

<!-- TODO(important): call out parts 5 and 6 -->

[Part 4](/blog/posts/slml_part4.qmd) covers how the dataset I used to train my first model contained drawings with too many short strokes, as in @fig-sepstrokes. Models trained on this dataset produced frenetic drawings such as @fig-before-sse. I experimented with some RNN training improvements and after filtering short strokes out of my first datasets and training a new model I saw a big improvment, as in @fig-after-sse.

:::{#fig-i16-results .column-body-outset layout-ncol=2}
![Before Short-Stroke Exclusion](https://i.ibb.co/Nnrn9jL/phase1-sample-0111-epoch-00330-decoded.png){#fig-before-sse}

![After Short-Stroke Exclusion](https://i.ibb.co/jzgHHPS/phase2-sample-0145-epoch-11500-decoded.png){#fig-after-sse}

Sample generated results from models trained before and after filtering out strokes with less than 10 points.
:::

<!--
:::{#fig-i16-results .column-screen-inset layout-ncol=3}
![](https://i.ibb.co/Nnrn9jL/phase1-sample-0111-epoch-00330-decoded.png)

![](https://i.ibb.co/fSnW50p/phase1-sample-0115-epoch-00310-decoded.png)

![](https://i.ibb.co/LNdGhBb/phase1-sample-0177-epoch-06000-decoded.png)

Generated outputs from SketchRNN trained on dataset `look_i16`.
:::
:::{#fig-i16-results .column-screen-inset layout-ncol=3}
![](https://i.ibb.co/hXBK8Zb/phase2-sample-0066-epoch-11700-decoded.png)

![](https://i.ibb.co/jzgHHPS/phase2-sample-0145-epoch-11500-decoded.png)

![](https://i.ibb.co/0mwbJBw/phase2-sample-0291-epoch-13200-decoded.png)

Generated outputs from SketchRNN trained on dataset `look_i16__minn10`.
:::
-->

[Part 5](/blog/posts/slml_part5.qmd) covers improvements I made to my preprocessing of the dataset by [joining SVG paths](https://singleline-dataset.andrewlook.com/strokes.html) up into continuous lines so that the model could learn from longer sequences.

:::{#fig-joining .column-body-outset layout-ncol=3}
![Original: 22 strokes](https://i.ibb.co/s21gLTQ/stroke3-01-sepstrokes.png){#fig-original}

![After path-joining: 4 strokes](https://i.ibb.co/602dKpr/stroke3-01-joinedstrokes.png){#fig-joined}

![After path-splicing: 2 strokes](https://i.ibb.co/rtnZMq1/stroke3-01-splicedstrokes1.png){#fig-spliced}

Comparison of number of strokes before and after preprocessing, with one color per stroke.
:::

<!--
Here's a debug video I made to flip through part of my dataset and watch drawings go from multiple strokes (one color per stroke) to a single stroke:

:::{.column-body}
{{< video "https://storage.googleapis.com/andrewlook-art-assets/andrewlook.com/videos/new_file_fps20.mp4" height="400" >}}
:::
-->

The generated results at this stage were uncanny, but showed a big improvement from the initial results. There were at least some recognizable faces and bodies, as seen in @fig-gen-spliced.

:::{#fig-gen-spliced .column-screen-inset layout-ncol=3}
![](https://i.ibb.co/ZGLMZ4H/phase3-runid-dhzx8671-epoch-00200-sample-0338-decoded.png)

![](https://i.ibb.co/5TVZG6H/phase3-runid-dhzx8671-epoch-00400-sample-0338-decoded.png)

![](https://i.ibb.co/KxRj62q/phase3-runid-093xwcex-epoch-01000-sample-0061-decoded.png)

Generated samples from a model trained on path-spliced dataset.
:::

Also in Part 5, I a preprocessing step to decompose strokes into seperate drawings if their bounding boxes didn't overlap sufficiently. The size of the dataset roughly doubled, since sketchbook pages containing multiple drawings were broken out into separate training examples as in @fig-bbox-split-example.

:::{#fig-bbox-split-example .column-screen-inset layout-ncol=4}
![Original](https://i.ibb.co/SrpsMWZ/bbox-20.png){#fig-prebbox}

![](https://i.ibb.co/PNLqQLG/bbox-21.png){#fig-postbbox-1}

![](https://i.ibb.co/wWKK6zC/bbox-22.png){#fig-postbbox-2}

![](https://i.ibb.co/N18YMC0/bbox-23.png){#fig-postbbox-3}

Original drawing (left) was one row in dataset `v2-splicedata`. The rightmost three drawings are distinct rows in dataset `20231214`.
:::

[Part 6](/blog/posts/slml_part6.qmd) covers how I filtered drawings by visual similarity. Though I did an earlier pass on visual similarity to filter watercolors out from my single-line drawings, this time I wanted to explore the embedding space of the individual drawings after bounding-box separation had been applied. I found that clustering the drawing embeddings identified clusters I wanted to exlcude from the training data like @fig-c0 and @fig-c13, and helped me identify clusters that I wanted to include in the training data like @fig-c5.

:::{#fig-chains .column-screen-inset layout-ncol=3}
![Cluster of complex drawings I wanted to filter out](https://i.ibb.co/ZghFLD3/c0.png){#fig-c0}

![Cluster of simple drawings I wanted to filter out](https://i.ibb.co/mBnwc2N/c13.png){#fig-c13}

![Cluster of good drawings I wanted to include](https://i.ibb.co/3MsTpmc/c5.png){#fig-c5}

Example groupings after running K-Means on visual embeddings of individual drawings.
:::

The drawings generated by models trained on this visually-filtered dataset started to look recognizable as containing distinct people or faces, as in @fig-gen-bboxsep. Still odd and convoluted, but interesting enough to give me new ideas for drawings or paintings.

:::{#fig-gen-bboxsep .column-screen-inset layout-ncol=3}
![](https://i.ibb.co/85p1XNk/phase5-test.png)

![](https://i.ibb.co/R41PJTS/phase5-test2.png)

![](https://i.ibb.co/SymDvdW/phase5-test3.png)

Generated samples after training with visual filtering on bbox-separated dataset.
:::

[Part 7](/blog/posts/slml_part7.qmd) covers experiments I tried using data augmentation, using tricks to take my existing set of drawings and create a more diverse set of training data. For regular computer vision algorithms looking at images in terms of pixels, it's common to randomly crop the images, flip them horizontally, and change the colors. For vector drawings like I'm working with, there are a different set of techniques available.

:::{#fig-data-aug .column-body-outset layout-ncol=3}
![Original drawing](https://i.ibb.co/sq9YtDX/aug0.png)

![After "Stroke Augmentation" drops points from lines at random](https://i.ibb.co/VDp5d3D/aug1.png)

![Randomly rotated and scaled](https://i.ibb.co/z59gv5S/aug2.png)

Examples of data augmentation.
:::



## Interpreting the Results

<!-- TODO(important): split into a separate analysis section? -->

It has also been fun to connect the model's generation function to an animation builder, so I can watch the machine "draw" in real time. Compared with viewing a static output, the animations reminds me that part of what I love about single-line drawings is the surprise as a viewer. 

{{< video "https://storage.googleapis.com/andrewlook-art-assets/andrewlook.com/videos/video-8.mp4" height="400" >}}

The drawing might start off nonsensical, and then end up with something recognizable enough to be an abstract figure drawing. Even when I'm drawing, I don't always know where the drawing is going to end up. 

{{< video "https://storage.googleapis.com/andrewlook-art-assets/andrewlook.com/videos/video-11.mp4" height="400" >}}

I'm adjusting as my hand moves, and adapting to any unexpected moves or mistakes to try and arrive at a drawing that I like. This is not so different from how SketchRNN generates drawings. One way to look at it is that I'm sampling from a probability distribution of possible next moves, and my decision is made by the muscle memory of what's happened so far.

Looking at some of the results generated from my SketchRNN models such as @fig-gen-bboxsep, they remind me of an experiment I've tried in drawing with my eyes closed.

I was curious about how much I'm adjusting drawings based on what I see while I'm drawing. I wanted to see how much of my drawings come exclusively from the muscle memory I've developed in drawing faces.

Drawing with eyes closed is a great analogy for how SketchRNN draws. The model is only receiving information about the path has traveled so far. No visual information is available about what the final drawing looks like in pixel space as a real image.

:::{.column-screen-inset layout-ncol=3}
![](https://i.ibb.co/xGVJtrL/067.jpg)

![](https://i.ibb.co/2jMwq12/sb66p089-eyesclosed.jpg)

![](https://i.ibb.co/vcwnvTX/sb66p091-eyesclosed.jpg)
<!-- 
![](https://i.ibb.co/f2YJ55v/065.jpg)

![](https://i.ibb.co/LPHSH29/066.jpg)
-->
:::

Errors like the ones in my eyes-closed drawings made me think about a common issue with models like SketchRNN that rely on recurrent neural networks. 

The problem of ["long-term dependencies"](https://ai-master.gitbooks.io/recurrent-neural-network/content/the-problem-of-long-term-dependencies.html) refers to the poor performance RNN's exhibit in understand things that are too far apart in a sequence. 

In the case of a drawing, long term dependencies would be things that are far apart in terms of the path the pen takes.

![Recurrent neural networks read a sequence and update the "hidden" layer. The hidden layers act as a kind of memory, allowing later steps in the sequence to incorporate information from earlier parts of the sequence. Each step incorporates more information to the same vector before passing it to the next step in the sequnce. As the RNN model steps through the sequence, the signal from earlier in the sequence tends to "decay" in favor of more recent information later in the sequence.](https://ai-master.gitbooks.io/recurrent-neural-network/content/assets/RNN_connection.jpg)

The long-term dependency problem makes intuitive sense to me when I consider my eyes-closed drawings.

Apparently I have muscle memory when I draw eyes and a note in close proximity, and in drawings lips and a chin, but without looking it's hard to swoop down from eyes to lips and have them be aligned to the nose I drew earlier.

![](https://i.ibb.co/4ZB8g6x/eyesclosed-explainer.png)

<!--
| ![phase5-test](https://i.ibb.co/85p1XNk/phase5-test.png) | ![phase5-test2](https://i.ibb.co/R41PJTS/phase5-test2.png) |
| ---- | ---- |
| ![phase5-test3](https://i.ibb.co/SymDvdW/phase5-test3.png) | ![phase5-test4](https://i.ibb.co/xqfCt4n/phase5-test4.png) |
| ![phase5-test5](https://i.ibb.co/cbh6D3F/phase5-test5.png) |  |
{{< video "https://storage.googleapis.com/andrewlook-art-assets/andrewlook.com/videos/video-11.mp4" >}}
{{< video "https://storage.googleapis.com/andrewlook-art-assets/andrewlook.com/videos/video-9.mp4" >}}
-->

This got me interested in how [Transformer models](https://jalammar.github.io/illustrated-transformer/) use attention mechanisms to let each step in the sequence take into account the entire sequence at once.

![](https://jalammar.github.io/images/t/transformer_self-attention_visualization.png)

I came across a paper called [Sketchformer: Transformer-based Representation for Sketched Structure](https://arxiv.org/abs/2002.10381), which made a transformer model based on SketchRNN. I decided to try adapting that model for my dataset, and seeing how it compares on handling long-term dependencies.

:::{.column-body-outset cap-location=bottom}
![Architecture of the SketchFormer model.](https://i.ibb.co/rx1nbmt/sketchformer-architecture.png)
:::

In my next section on "Training Sketchformer" _(coming soon)_, I cover my experiments using a transformer model instead of an RNN, to see if the model can better handle long-term dependencies within the drawings. 